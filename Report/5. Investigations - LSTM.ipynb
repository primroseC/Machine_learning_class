{"cells":[{"cell_type":"markdown","source":["#MA707 Report - Investigation - LSTM and Sequential (spring 2019, Blackjack)"],"metadata":{}},{"cell_type":"markdown","source":["## Contents\n1. Disclaimer\n2. LSTM Model\n3. Sequential Model"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Disclaimer"],"metadata":{}},{"cell_type":"markdown","source":["NOTE: This notebook stands separately from the other notebooks currently written in that it is purely our guesswork and attempt to make a model that is unique.  In terms of full disclosure, some of the code in this notebook is \"foreign\" to us in that we do not fully understand its functionality, but more how to coerce it to work and produce results.  We looked to utilize LSTM modelling and explored the Sequential Keras node and how to potentially wrap it within a class."],"metadata":{}},{"cell_type":"code","source":["%run \"./1. Class demonstrations\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from math import sqrt\nfrom numpy import concatenate\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom keras.layers import LSTM"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Using TensorFlow backend.\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["The above imports the notebooks we will need - note that we need a special cluster connection for the keras library to run here, so be sure to connect this notebook to the class.ma707.test cluster"],"metadata":{}},{"cell_type":"markdown","source":["The below is the same code written earlier that allows us to take our grid search arguments and read them in as a data frame and include only in the important parts of the grid search."],"metadata":{}},{"cell_type":"code","source":["def display_pdf(a_pdf):\n  display(spark.createDataFrame(a_pdf,verifySchema=False))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["def est_grid_results_pdf(my_est_grid_obj,est_tag=None,fea_tag=None): \n  import pandas as pd\n  import numpy  as np\n  res_pdf = pd.DataFrame(data=my_est_grid_obj.cv_results_) \\\n           .loc[:,lambda df: np.logical_or(df.columns.str.startswith('param_'),\n                                           df.columns.str.endswith('test_score'))\n               ] \\\n           .loc[:,lambda df: np.logical_not(df.columns.str.startswith('split'))\n               ] \\\n           .drop(['rank_test_score', 'std_test_score'], \n                 axis=1)\n  res_pdf.columns = [column.replace('param_','') for column in list(res_pdf.columns)]\n  if est_tag is not None: res_pdf = res_pdf.assign(est_tag=est_tag)\n  if fea_tag is not None: res_pdf = res_pdf.assign(fea_tag=fea_tag)\n  return res_pdf.sort_values('mean_test_score', ascending = False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Code to create train and test data, see previous code in Notebook #3 for more in depth explanation"],"metadata":{}},{"cell_type":"code","source":["def create_train_test_ts(fea_pdf, tgt_ser, trn_prop=0.8):\n  trn_len = int(trn_prop * len(fea_pdf))\n  return (fea_pdf.iloc[:trn_len],\n          fea_pdf.iloc[ trn_len:],\n          tgt_ser.iloc[:trn_len],\n          tgt_ser.iloc[ trn_len:]\n         )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["The three code blocks below can be used to create plots for viewing how the predicted code works agains thte actual result"],"metadata":{}},{"cell_type":"code","source":["def plot_comparison(test_p, test_y):\n  pyplot.clf()\n  pyplot.plot(test_y, label='actual')\n  pyplot.plot(test_p, label='predicted')\n  pyplot.legend()\n  display(pyplot.show())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["def plot_history():\n  pyplot.clf()\n  pyplot.plot(history.history['loss']    , label='train')\n  pyplot.plot(history.history['val_loss'], label='test')\n  pyplot.legend()\n  display(pyplot.show())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["def plot_actual(target_ser):\n  import numpy as np\n  pyplot.clf()\n  pyplot.plot(target_ser,label='actual')\n  pyplot.legend()\n  display(pyplot.show())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["## 2. LSTM"],"metadata":{}},{"cell_type":"markdown","source":["The code below (as we understand it) is based off code provided in class, but with some modified features. There is an interest in testing the different optimization, activation, and loss functions for the LSTM model, so we allow the user to enter these as input. By doing this, the hope is that we can mix and match a perfect combination of these from a subset to figure out what works best for the given data frame.  The model works as follows - the init parameter takes as argument (besides those already mentioned), the number of epochs, the batch size, and the number of units to be considered and assigns these to the self-function.  It specifies the model to be used as Sequential, which is simply the type of model we will be building is specified for us.  The fit method first runs through a min max scaler for the x and. y variable, and then assigns these values to be x and y scaled.  The x variable is than reshaped, so that it is a three-dimensional array, in this case with 1 sample, 1 iteration, and 1 feature at each point.  We then add the LSTM functionality to the Sequential model, and assign the number of units, and the input shape to the LSTM.  We add a dense layer and specify that it only have one output, and this is where we define the activation function for the output.  We then compile the model, using our specified loss and optimization measures from the init method, and use the fit method of the Sequential model with the necessary inputs.  The predict method given allows us to generate predictions based on the given methods and will be used with the different grid search parameters ran. NOTE: After trying to add in the hyperparameters for activation, optimization and loss, we got an error, because it is not an argument for the model. Fit method of sequential, see attached link: [Link Machine Learning](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)"],"metadata":{}},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, RegressorMixin\nfrom keras.layers import LSTM\nclass LSTMWrapper_failure(BaseEstimator,RegressorMixin,LSTM):\n  def __init__(self,verbose=0,epochs=1,batch_size=1,nb_units=50, loss_str, optimizer_str,  activation_str:\n    self.model       = Sequential()\n    self.verbose     = verbose\n    self.epochs      = epochs\n    self.batch_size  = batch_size\n    self.nb_units    = nb_units\n    self.loss        = loss_str\n    self.optimizer   = optimizer_str\n    self.activation  = activation_Str\n    return \n  \n  def fit(self,X,y=None):\n    from sklearn.preprocessing import MinMaxScaler\n    self.scl_X = MinMaxScaler(feature_range=(0, 1))\n    self.scl_y = MinMaxScaler(feature_range=(0, 1))\n    X_scl = self.scl_X.fit_transform(X)\n    y_scl = self.scl_y.fit_transform(y.reshape(-1,1))\n    X_scl_re = X_scl.reshape((X_scl.shape[0], 1, X_scl.shape[1]))\n    self.model.add(LSTM(self.nb_units, \n                        input_shape=(X_scl_re.shape[1], X_scl_re.shape[2])))\n    self.model.add(Dense(1,activation = self.activation))\n    self.model.compile(loss= self.loss, optimizer= 'adam')\n    self.model.fit(X_scl_re, y_scl, \n                   epochs    =self.epochs, \n                   batch_size=self.batch_size, \n                   verbose   =self.verbose, \n                   shuffle   =False)\n    return self\n  \n  def predict(self,X,y=None):\n    X_scl    = self.scl_X.transform(X)\n    X_scl_re = X_scl.reshape((X_scl.shape[0], 1, X_scl.shape[1]))\n    return self.scl_y.inverse_transform(self.model.predict(X_scl_re))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-1465258&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">6</span>\n<span class=\"ansi-red-fg\">    self.verbose     = verbose</span>\n       ^\n<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> invalid syntax\n</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["Per above article, and other article, we determined that we cannot in fact accomplish the model tuning in the way we wanted to, but we can tune the number of neurons for our given dataset.  We restate the original code for thee wrapper class, but with some additional parameters.  Dropout helps to reduce overfitting, making our model more useful for testing data, we also note that, despite several attempts (above and elsewhere) that in order to make a grid search for loss, optimization and such we would need a separate wrapper class for our Sequential model, as those hyperparameters are not part of the LSTM input. [Hyperparameter LSTM](https://machinelearningmastery.com/tune-lstm-hyperparameters-keras-time-series-forecasting/)"],"metadata":{}},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, RegressorMixin\nfrom keras.layers import LSTM\nclass LSTMWrapper_w_dropout(BaseEstimator,RegressorMixin,LSTM):\n  def __init__(self,verbose=0,epochs=1,batch_size=1,nb_units=50):\n    self.model       = Sequential()\n    self.verbose     = verbose\n    self.epochs      = epochs\n    self.batch_size  = batch_size\n    self.nb_units    = nb_units\n    return \n  \n  def fit(self,X,y=None):\n    from sklearn.preprocessing import MinMaxScaler\n    self.scl_X = MinMaxScaler(feature_range=(0, 1))\n    self.scl_y = MinMaxScaler(feature_range=(0, 1))\n    X_scl = self.scl_X.fit_transform(X)\n    y_scl = self.scl_y.fit_transform(y.reshape(-1,1))\n    X_scl_re = X_scl.reshape((X_scl.shape[0], 1, X_scl.shape[1]))\n    self.model.add(LSTM(self.nb_units, \n                        input_shape=(X_scl_re.shape[1], X_scl_re.shape[2])))\n    self.model.add (Dropout(0.2))\n    self.model.add(Dense(1))\n    self.model.compile(loss='mae', optimizer='adam')\n    self.model.fit(X_scl_re, y_scl, \n                   epochs    =self.epochs, \n                   batch_size=self.batch_size, \n                   verbose   =self.verbose, \n                   shuffle   =False)\n    return self\n  \n  def predict(self,X,y=None):\n    X_scl    = self.scl_X.transform(X)\n    X_scl_re = X_scl.reshape((X_scl.shape[0], 1, X_scl.shape[1]))\n    return self.scl_y.inverse_transform(self.model.predict(X_scl_re))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["the below code is a simple preprocessing pipeline to create something to be used with our LSTM model.  It contains only the bci data frame, as we are not going to work on NLP with this model.  Note we will create two of these, one for lag 3 and one for lag 7 - w will compare how the two models differ from each other through the hyperparameter selection process"],"metadata":{}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\nbci_pipe_lag3 = \\\nPipeline(steps=[\n    ('fea_one', FeatureUnionDF(transformer_list=[\n      ('tgt_var'     ,CreateTargetVarDF(var='bci_5tc')),\n      ('lag_bci_vars',CreateLagVarsDF(var_list=['c5', 'c7', 'p1a_03', 'p2a_03', 'p4_03', 'p3a_iv', 'shfe_al3',\n                                                'rici', 'ice_kc3', 'cme_sm3', 'cme_lc2', 'opec_orb', 'shfe_cu3',\n                                                'cme_ln1', 'cme_fc3', 'p3a_03', 'shfe_rb3', 'cme_s2', 'ice_sb3',\n                                                'cme_ln3', 'cme_ln2', 'ice_tib3', 'ice_tib4', 'bci'],\n                                      lag_list=[3])),\n    ])),\n    ('drop_na_rows'  ,DropNaRowsDF(how='any'))\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\nbci_pipe_lag7 = \\\nPipeline(steps=[\n    ('fea_one', FeatureUnionDF(transformer_list=[\n      ('tgt_var'     ,CreateTargetVarDF(var='bci_5tc')),\n      ('lag_bci_vars',CreateLagVarsDF(var_list=['c5', 'c7', 'p1a_03', 'p2a_03', 'p4_03', 'p3a_iv', 'shfe_al3',\n                                                'rici', 'ice_kc3', 'cme_sm3', 'cme_lc2', 'opec_orb', 'shfe_cu3',\n                                                'cme_ln1', 'cme_fc3', 'p3a_03', 'shfe_rb3', 'cme_s2', 'ice_sb3',\n                                                'cme_ln3', 'cme_ln2', 'ice_tib3', 'ice_tib4', 'bci'],\n                                      lag_list=[7])),\n    ])),\n    ('drop_na_rows'  ,DropNaRowsDF(how='any'))\n  ])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["We create the necessary train/test datasets and prepare to begin a grid search on the hyperparameters"],"metadata":{}},{"cell_type":"code","source":["fea_tgt_pdf_lag3 = bci_pipe_lag3.fit_transform(bci_pdf)\n(trn_fea_pdf_3, tst_fea_pdf_3, \n trn_tgt_ser_3, tst_tgt_ser_3\n) = \\\ncreate_train_test_ts(fea_pdf = fea_tgt_pdf_lag3.drop( 'target',axis=1),\n                     tgt_ser = fea_tgt_pdf_lag3.loc[:,'target'],\n                     trn_prop= 0.9\n                    )\n(trn_fea_pdf_3.shape, tst_fea_pdf_3.shape, \n trn_tgt_ser_3.shape, tst_tgt_ser_3.shape\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[61]: ((1439, 24), (160, 24), (1439,), (160,))\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["fea_tgt_pdf_lag7 = bci_pipe_lag7.fit_transform(bci_pdf)\n(trn_fea_pdf_7, tst_fea_pdf_7, \n trn_tgt_ser_7, tst_tgt_ser_7\n) = \\\ncreate_train_test_ts(fea_pdf = fea_tgt_pdf_lag7.drop( 'target',axis=1),\n                     tgt_ser = fea_tgt_pdf_lag7.loc[:,'target'],\n                     trn_prop= 0.9\n                    )\n(trn_fea_pdf_7.shape, tst_fea_pdf_7.shape, \n trn_tgt_ser_7.shape, tst_tgt_ser_7.shape\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[62]: ((1435, 24), (160, 24), (1435,), (160,))\n</div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["Grid search Time! we will begin with a simple search performed on the lag 3 variable, and will work to tune the parameters for lag 3 and lag 7.  Note because of above realizations, we will not be able to tune thee optimization, activation and loss functions through the grid search, but will be able to observe changes in number of units, epochs, batch size and neurons."],"metadata":{}},{"cell_type":"code","source":["from spark_sklearn           import GridSearchCV\nfrom sklearn.pipeline        import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics         import make_scorer, mean_absolute_error, r2_score\nsimple_gs = \\\nGridSearchCV(sc,\n             estimator=Pipeline(steps=[('lstm',LSTMWrapper_w_dropout(verbose=2))\n                                      ]),\n             param_grid={'lstm__nb_units':[10,100],\n                         'lstm__epochs':[20,50],\n                         'lstm__batch_size':[10,50],\n                        },\n             cv=TimeSeriesSplit(n_splits=2),\n             scoring=make_scorer(r2_score),\n             return_train_score=False,\n             n_jobs=-1 \n            ) \nsimple_gs \\\n  .fit(trn_fea_pdf_3.values,\n       trn_tgt_ser_3.values)\ndisplay_pdf(est_grid_results_pdf(simple_gs,\n                                 est_tag='lstm'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mean_test_score</th><th>lstm__batch_size</th><th>lstm__epochs</th><th>lstm__nb_units</th><th>est_tag</th></tr></thead><tbody><tr><td>0.8032430591530905</td><td>10</td><td>50</td><td>10</td><td>lstm</td></tr><tr><td>0.726682879856092</td><td>50</td><td>50</td><td>100</td><td>lstm</td></tr><tr><td>0.6785707281692293</td><td>10</td><td>20</td><td>10</td><td>lstm</td></tr><tr><td>0.6697516025576068</td><td>10</td><td>50</td><td>100</td><td>lstm</td></tr><tr><td>0.6277807480075019</td><td>50</td><td>50</td><td>10</td><td>lstm</td></tr><tr><td>0.4510614531340376</td><td>50</td><td>20</td><td>10</td><td>lstm</td></tr><tr><td>0.28092022608842215</td><td>50</td><td>20</td><td>100</td><td>lstm</td></tr><tr><td>-1.4021268117554886</td><td>10</td><td>20</td><td>100</td><td>lstm</td></tr></tbody></table></div>"]}}],"execution_count":29},{"cell_type":"code","source":["from spark_sklearn           import GridSearchCV\nfrom sklearn.pipeline        import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics         import make_scorer, mean_absolute_error, r2_score\nsimple_gs = \\\nGridSearchCV(sc,\n             estimator=Pipeline(steps=[('lstm',LSTMWrapper_w_dropout(verbose=2))\n                                      ]),\n             param_grid={'lstm__nb_units':[10,100],\n                         'lstm__epochs':[20,50],\n                         'lstm__batch_size':[10,50],\n                        },\n             cv=TimeSeriesSplit(n_splits=2),\n             scoring=make_scorer(r2_score),\n             return_train_score=False,\n             n_jobs=-1 \n            ) \nsimple_gs \\\n  .fit(trn_fea_pdf_7.values,\n       trn_tgt_ser_7.values)\ndisplay_pdf(est_grid_results_pdf(simple_gs,\n                                 est_tag='lstm'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mean_test_score</th><th>lstm__batch_size</th><th>lstm__epochs</th><th>lstm__nb_units</th><th>est_tag</th></tr></thead><tbody><tr><td>0.6424707216206376</td><td>50</td><td>50</td><td>100</td><td>lstm</td></tr><tr><td>0.6132351910186489</td><td>10</td><td>20</td><td>100</td><td>lstm</td></tr><tr><td>0.5579599995560757</td><td>50</td><td>50</td><td>10</td><td>lstm</td></tr><tr><td>0.556669210321278</td><td>50</td><td>20</td><td>100</td><td>lstm</td></tr><tr><td>0.49041395266784366</td><td>10</td><td>20</td><td>10</td><td>lstm</td></tr><tr><td>0.48916443961917233</td><td>10</td><td>50</td><td>10</td><td>lstm</td></tr><tr><td>0.43109844514489415</td><td>10</td><td>50</td><td>100</td><td>lstm</td></tr><tr><td>0.3519033012324962</td><td>50</td><td>20</td><td>10</td><td>lstm</td></tr></tbody></table></div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["We observe from above that the 7 day lagged variables produce a worse mean test score, but all focus on the higher numbers specified in our parameter grid.  The 3-day lag variables only have the epochs in common with the lag 7.  We will continue by changing some hyper parameters to be a bit more accurate"],"metadata":{}},{"cell_type":"code","source":["from spark_sklearn           import GridSearchCV\nfrom sklearn.pipeline        import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics         import make_scorer, mean_absolute_error, r2_score\nsimple_gs = \\\nGridSearchCV(sc,\n             estimator=Pipeline(steps=[('lstm',LSTMWrapper_w_dropout(verbose=2))\n                                      ]),\n             param_grid={'lstm__nb_units':[10,100],\n                         'lstm__epochs':[50,100],\n                         'lstm__batch_size':[10,50],\n                        },\n             cv=TimeSeriesSplit(n_splits=2),\n             scoring=make_scorer(r2_score),\n             return_train_score=False,\n             n_jobs=-1 \n            ) \nsimple_gs \\\n  .fit(trn_fea_pdf_3.values,\n       trn_tgt_ser_3.values)\ndisplay_pdf(est_grid_results_pdf(simple_gs,\n                                 est_tag='lstm'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mean_test_score</th><th>lstm__batch_size</th><th>lstm__epochs</th><th>lstm__nb_units</th><th>est_tag</th></tr></thead><tbody><tr><td>0.7328298996954958</td><td>10</td><td>100</td><td>10</td><td>lstm</td></tr><tr><td>0.716160934953555</td><td>50</td><td>50</td><td>100</td><td>lstm</td></tr><tr><td>0.6409753557453437</td><td>50</td><td>100</td><td>100</td><td>lstm</td></tr><tr><td>0.5849150397498094</td><td>10</td><td>50</td><td>100</td><td>lstm</td></tr><tr><td>0.5102485408877465</td><td>10</td><td>50</td><td>10</td><td>lstm</td></tr><tr><td>0.36660806696728687</td><td>50</td><td>50</td><td>10</td><td>lstm</td></tr><tr><td>0.29331434164220554</td><td>50</td><td>100</td><td>10</td><td>lstm</td></tr><tr><td>-2.8344356258771026</td><td>10</td><td>100</td><td>100</td><td>lstm</td></tr></tbody></table></div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["Interestingly enough, our top result is now worse than it was before, most likely related to the dropout parameter - there is also huge loss here from the sheer time required to run the many epochs, therefore we will go back to the original model of epochs and attempt to find more accurate batch sizes and number of units"],"metadata":{}},{"cell_type":"code","source":["from spark_sklearn           import GridSearchCV\nfrom sklearn.pipeline        import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics         import make_scorer, mean_absolute_error, r2_score\nsimple_gs = \\\nGridSearchCV(sc,\n             estimator=Pipeline(steps=[('lstm',LSTMWrapper_w_dropout(verbose=2))\n                                      ]),\n             param_grid={'lstm__nb_units':[5,10],\n                         'lstm__epochs':[50],\n                         'lstm__batch_size':[5,10],\n                        },\n             cv=TimeSeriesSplit(n_splits=2),\n             scoring=make_scorer(r2_score),\n             return_train_score=False,\n             n_jobs=-1 \n            ) \nsimple_gs \\\n  .fit(trn_fea_pdf_3.values,\n       trn_tgt_ser_3.values)\ndisplay_pdf(est_grid_results_pdf(simple_gs,\n                                 est_tag='lstm'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mean_test_score</th><th>lstm__batch_size</th><th>lstm__epochs</th><th>lstm__nb_units</th><th>est_tag</th></tr></thead><tbody><tr><td>0.8112314577009487</td><td>10</td><td>50</td><td>10</td><td>lstm</td></tr><tr><td>0.5404755479812213</td><td>5</td><td>50</td><td>10</td><td>lstm</td></tr><tr><td>-1.4707169035885315</td><td>10</td><td>50</td><td>5</td><td>lstm</td></tr><tr><td>-2.672992942228093</td><td>5</td><td>50</td><td>5</td><td>lstm</td></tr></tbody></table></div>"]}}],"execution_count":34},{"cell_type":"markdown","source":["We see above that the model that is most accurate is unchanged - therefore we will try one more time to get better results, by marginally increasing batch size and number of units, otherwise we will conclude that the original model of 10,50,10 is the best model for lag 3"],"metadata":{}},{"cell_type":"code","source":["from spark_sklearn           import GridSearchCV\nfrom sklearn.pipeline        import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics         import make_scorer, mean_absolute_error, r2_score\nsimple_gs = \\\nGridSearchCV(sc,\n             estimator=Pipeline(steps=[('lstm',LSTMWrapper_w_dropout(verbose=2))\n                                      ]),\n             param_grid={'lstm__nb_units':[10,20],\n                         'lstm__epochs':[50],\n                         'lstm__batch_size':[20,10],\n                        },\n             cv=TimeSeriesSplit(n_splits=2),\n             scoring=make_scorer(r2_score),\n             return_train_score=False,\n             n_jobs=-1 \n            ) \nsimple_gs \\\n  .fit(trn_fea_pdf_3.values,\n       trn_tgt_ser_3.values)\ndisplay_pdf(est_grid_results_pdf(simple_gs,\n                                 est_tag='lstm'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mean_test_score</th><th>lstm__batch_size</th><th>lstm__epochs</th><th>lstm__nb_units</th><th>est_tag</th></tr></thead><tbody><tr><td>0.5505666315589425</td><td>10</td><td>50</td><td>20</td><td>lstm</td></tr><tr><td>0.5074810192806085</td><td>10</td><td>50</td><td>10</td><td>lstm</td></tr><tr><td>0.4773660954592912</td><td>20</td><td>50</td><td>20</td><td>lstm</td></tr><tr><td>0.3244580617638214</td><td>20</td><td>50</td><td>10</td><td>lstm</td></tr></tbody></table></div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["Based on above, the one thing we can conclude, is that we can’t conclude anything with confidence right now.  It is possible that the models are not working as well as a result of the additional dropout.  Given that we have yet to beat a model from our first one we are now concluding that this model is going to work, and therefore are going to try and improve our model for lag 7."],"metadata":{}},{"cell_type":"code","source":["from spark_sklearn           import GridSearchCV\nfrom sklearn.pipeline        import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics         import make_scorer, mean_absolute_error, r2_score\nsimple_gs = \\\nGridSearchCV(sc,\n             estimator=Pipeline(steps=[('lstm',LSTMWrapper_w_dropout(verbose=2))\n                                      ]),\n             param_grid={'lstm__nb_units':[100,200],\n                         'lstm__epochs':[20,50],\n                         'lstm__batch_size':[50,100],\n                        },\n             cv=TimeSeriesSplit(n_splits=2),\n             scoring=make_scorer(r2_score),\n             return_train_score=False,\n             n_jobs=-1 \n            ) \nsimple_gs \\\n  .fit(trn_fea_pdf_7.values,\n       trn_tgt_ser_7.values)\ndisplay_pdf(est_grid_results_pdf(simple_gs,\n                                 est_tag='lstm'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mean_test_score</th><th>lstm__batch_size</th><th>lstm__epochs</th><th>lstm__nb_units</th><th>est_tag</th></tr></thead><tbody><tr><td>0.6482755039627647</td><td>50</td><td>50</td><td>100</td><td>lstm</td></tr><tr><td>0.6455213225541521</td><td>100</td><td>50</td><td>200</td><td>lstm</td></tr><tr><td>0.6425760584910437</td><td>50</td><td>50</td><td>200</td><td>lstm</td></tr><tr><td>0.6286908515253123</td><td>100</td><td>20</td><td>200</td><td>lstm</td></tr><tr><td>0.6251739476010353</td><td>100</td><td>50</td><td>100</td><td>lstm</td></tr><tr><td>0.5939287504492248</td><td>50</td><td>20</td><td>100</td><td>lstm</td></tr><tr><td>0.5888666152735456</td><td>100</td><td>20</td><td>100</td><td>lstm</td></tr><tr><td>0.5844254888722884</td><td>50</td><td>20</td><td>200</td><td>lstm</td></tr></tbody></table></div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["We have no exact change from the model above on a 7 day lag, therefore we will try considering unit sizes between 100 and 50 and batch sizes between 20 and 50, we will standardize epochs to increase run speed here"],"metadata":{}},{"cell_type":"code","source":["from spark_sklearn           import GridSearchCV\nfrom sklearn.pipeline        import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics         import make_scorer, mean_absolute_error, r2_score\nsimple_gs = \\\nGridSearchCV(sc,\n             estimator=Pipeline(steps=[('lstm',LSTMWrapper_w_dropout(verbose=2))\n                                      ]),\n             param_grid={'lstm__nb_units':[75,100],\n                         'lstm__epochs':[50],\n                         'lstm__batch_size':[35,50],\n                        },\n             cv=TimeSeriesSplit(n_splits=2),\n             scoring=make_scorer(r2_score),\n             return_train_score=False,\n             n_jobs=-1 \n            ) \nsimple_gs \\\n  .fit(trn_fea_pdf_7.values,\n       trn_tgt_ser_7.values)\ndisplay_pdf(est_grid_results_pdf(simple_gs,\n                                 est_tag='lstm'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mean_test_score</th><th>lstm__batch_size</th><th>lstm__epochs</th><th>lstm__nb_units</th><th>est_tag</th></tr></thead><tbody><tr><td>0.6540663727083011</td><td>50</td><td>50</td><td>100</td><td>lstm</td></tr><tr><td>0.6514147063336818</td><td>50</td><td>50</td><td>75</td><td>lstm</td></tr><tr><td>0.6251044326217662</td><td>35</td><td>50</td><td>75</td><td>lstm</td></tr><tr><td>0.5166306088943846</td><td>35</td><td>50</td><td>100</td><td>lstm</td></tr></tbody></table></div>"]}}],"execution_count":40},{"cell_type":"markdown","source":["We achieve marginal improvement over the previous slide by limiting to 75 but see no other changes as it relates to batch size.  For the sake of trying, we will try running the epochs between 50/100 and standardize batch size to see the impacts."],"metadata":{}},{"cell_type":"code","source":["from spark_sklearn           import GridSearchCV\nfrom sklearn.pipeline        import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics         import make_scorer, mean_absolute_error, r2_score\nsimple_gs = \\\nGridSearchCV(sc,\n             estimator=Pipeline(steps=[('lstm',LSTMWrapper_w_dropout(verbose=2))\n                                      ]),\n             param_grid={'lstm__nb_units':[75,100],\n                         'lstm__epochs':[50, 100],\n                         'lstm__batch_size':[50],\n                        },\n             cv=TimeSeriesSplit(n_splits=2),\n             scoring=make_scorer(r2_score),\n             return_train_score=False,\n             n_jobs=-1 \n            ) \nsimple_gs \\\n  .fit(trn_fea_pdf_7.values,\n       trn_tgt_ser_7.values)\ndisplay_pdf(est_grid_results_pdf(simple_gs,\n                                 est_tag='lstm'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mean_test_score</th><th>lstm__batch_size</th><th>lstm__epochs</th><th>lstm__nb_units</th><th>est_tag</th></tr></thead><tbody><tr><td>0.6559488632443122</td><td>50</td><td>50</td><td>75</td><td>lstm</td></tr><tr><td>0.6503623391069803</td><td>50</td><td>50</td><td>100</td><td>lstm</td></tr><tr><td>0.6373476554139575</td><td>50</td><td>100</td><td>75</td><td>lstm</td></tr><tr><td>0.5656251061504227</td><td>50</td><td>100</td><td>100</td><td>lstm</td></tr></tbody></table></div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["Interestingly enough, we find that adding more epochs both slows down processing time and decreases the overall efficacy.  We will move forward with the final model as 50,50,75."],"metadata":{}},{"cell_type":"markdown","source":["Belwo, we run our selected models on the test data to see what the results we garner will be"],"metadata":{}},{"cell_type":"code","source":[" lstm_model_7 = \\\n  LSTMWrapper_w_dropout(nb_units=50,epochs=50,verbose=2,batch_size=75) \\\n    .fit(trn_fea_pdf_7.values,\n         trn_tgt_ser_7.values)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n  warnings.warn(msg, DataConversionWarning)\nEpoch 1/50\n - 2s - loss: 0.1931\nEpoch 2/50\n - 0s - loss: 0.1594\nEpoch 3/50\n - 0s - loss: 0.1174\nEpoch 4/50\n - 0s - loss: 0.1071\nEpoch 5/50\n - 0s - loss: 0.1016\nEpoch 6/50\n - 0s - loss: 0.0950\nEpoch 7/50\n - 0s - loss: 0.0927\nEpoch 8/50\n - 0s - loss: 0.0907\nEpoch 9/50\n - 0s - loss: 0.0893\nEpoch 10/50\n - 0s - loss: 0.0872\nEpoch 11/50\n - 0s - loss: 0.0870\nEpoch 12/50\n - 0s - loss: 0.0842\nEpoch 13/50\n - 0s - loss: 0.0829\nEpoch 14/50\n - 0s - loss: 0.0826\nEpoch 15/50\n - 0s - loss: 0.0826\nEpoch 16/50\n - 0s - loss: 0.0815\nEpoch 17/50\n - 0s - loss: 0.0794\nEpoch 18/50\n - 0s - loss: 0.0792\nEpoch 19/50\n - 0s - loss: 0.0766\nEpoch 20/50\n - 0s - loss: 0.0780\nEpoch 21/50\n - 0s - loss: 0.0781\nEpoch 22/50\n - 0s - loss: 0.0760\nEpoch 23/50\n - 0s - loss: 0.0744\nEpoch 24/50\n - 0s - loss: 0.0763\nEpoch 25/50\n - 0s - loss: 0.0758\nEpoch 26/50\n - 0s - loss: 0.0717\nEpoch 27/50\n - 0s - loss: 0.0744\nEpoch 28/50\n - 0s - loss: 0.0766\nEpoch 29/50\n - 0s - loss: 0.0711\nEpoch 30/50\n - 0s - loss: 0.0702\nEpoch 31/50\n - 0s - loss: 0.0752\nEpoch 32/50\n - 0s - loss: 0.0734\nEpoch 33/50\n - 0s - loss: 0.0704\nEpoch 34/50\n - 0s - loss: 0.0713\nEpoch 35/50\n - 0s - loss: 0.0736\nEpoch 36/50\n - 0s - loss: 0.0692\nEpoch 37/50\n - 0s - loss: 0.0675\nEpoch 38/50\n - 0s - loss: 0.0750\nEpoch 39/50\n - 0s - loss: 0.0718\nEpoch 40/50\n - 0s - loss: 0.0670\nEpoch 41/50\n - 0s - loss: 0.0723\nEpoch 42/50\n - 0s - loss: 0.0783\nEpoch 43/50\n - 0s - loss: 0.0673\nEpoch 44/50\n - 0s - loss: 0.0673\nEpoch 45/50\n - 0s - loss: 0.0818\nEpoch 46/50\n - 0s - loss: 0.0687\nEpoch 47/50\n - 0s - loss: 0.0655\nEpoch 48/50\n - 0s - loss: 0.0757\nEpoch 49/50\n - 0s - loss: 0.0723\nEpoch 50/50\n - 0s - loss: 0.0683\n</div>"]}}],"execution_count":45},{"cell_type":"code","source":[" lstm_model_3 = \\\n  LSTMWrapper_w_dropout(nb_units=10,epochs=50,verbose=2,batch_size=20) \\\n    .fit(trn_fea_pdf_3.values,\n         trn_tgt_ser_3.values)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n  warnings.warn(msg, DataConversionWarning)\nEpoch 1/50\n - 2s - loss: 0.1438\nEpoch 2/50\n - 0s - loss: 0.1466\nEpoch 3/50\n - 0s - loss: 0.1287\nEpoch 4/50\n - 0s - loss: 0.1150\nEpoch 5/50\n - 0s - loss: 0.1060\nEpoch 6/50\n - 0s - loss: 0.1029\nEpoch 7/50\n - 0s - loss: 0.0979\nEpoch 8/50\n - 0s - loss: 0.1013\nEpoch 9/50\n - 0s - loss: 0.0955\nEpoch 10/50\n - 0s - loss: 0.0938\nEpoch 11/50\n - 0s - loss: 0.0911\nEpoch 12/50\n - 0s - loss: 0.0914\nEpoch 13/50\n - 0s - loss: 0.0875\nEpoch 14/50\n - 0s - loss: 0.0837\nEpoch 15/50\n - 0s - loss: 0.0839\nEpoch 16/50\n - 0s - loss: 0.0815\nEpoch 17/50\n - 0s - loss: 0.0808\nEpoch 18/50\n - 0s - loss: 0.0805\nEpoch 19/50\n - 0s - loss: 0.0778\nEpoch 20/50\n - 0s - loss: 0.0776\nEpoch 21/50\n - 0s - loss: 0.0801\nEpoch 22/50\n - 0s - loss: 0.0771\nEpoch 23/50\n - 0s - loss: 0.0759\nEpoch 24/50\n - 0s - loss: 0.0688\nEpoch 25/50\n - 0s - loss: 0.0686\nEpoch 26/50\n - 0s - loss: 0.0719\nEpoch 27/50\n - 0s - loss: 0.0723\nEpoch 28/50\n - 0s - loss: 0.0687\nEpoch 29/50\n - 0s - loss: 0.0718\nEpoch 30/50\n - 0s - loss: 0.0669\nEpoch 31/50\n - 0s - loss: 0.0657\nEpoch 32/50\n - 0s - loss: 0.0648\nEpoch 33/50\n - 0s - loss: 0.0633\nEpoch 34/50\n - 0s - loss: 0.0637\nEpoch 35/50\n - 0s - loss: 0.0669\nEpoch 36/50\n - 0s - loss: 0.0634\nEpoch 37/50\n - 0s - loss: 0.0654\nEpoch 38/50\n - 0s - loss: 0.0672\nEpoch 39/50\n - 0s - loss: 0.0634\nEpoch 40/50\n - 0s - loss: 0.0614\nEpoch 41/50\n - 0s - loss: 0.0597\nEpoch 42/50\n - 0s - loss: 0.0617\nEpoch 43/50\n - 0s - loss: 0.0591\nEpoch 44/50\n - 0s - loss: 0.0637\nEpoch 45/50\n - 0s - loss: 0.0608\nEpoch 46/50\n - 0s - loss: 0.0616\nEpoch 47/50\n - 0s - loss: 0.0586\nEpoch 48/50\n - 0s - loss: 0.0602\nEpoch 49/50\n - 0s - loss: 0.0577\nEpoch 50/50\n - 0s - loss: 0.0588\n</div>"]}}],"execution_count":46},{"cell_type":"code","source":["lstm_model_3.score(tst_fea_pdf_3.values,\n                 tst_tgt_ser_3.values)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[66]: 0.7074176258871395\n</div>"]}}],"execution_count":47},{"cell_type":"code","source":["lstm_model_7.score(tst_fea_pdf_7.values,\n                 tst_tgt_ser_7.values)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[67]: 0.166258136457462\n</div>"]}}],"execution_count":48},{"cell_type":"markdown","source":["As we can see above - the lag 3 model worked rather well in predicting on test data, but the lag 7 model was horrendous.  As a result of this, we can conclude that the lag7 model was likely overfit, and that we should have considered a higher dropout rate to counteract this fact."],"metadata":{}},{"cell_type":"markdown","source":["###Sequential"],"metadata":{}},{"cell_type":"markdown","source":["The below is our greater attempt at determine the impact different activation/optimization/loss functions could have on our machine learning process. In the neural net we originally created we were unable to run a grid search on these parameters, because the class we wrapped, and the parameters were only for the LSTM class.  Below, we demonstrate that the Sequential model itself can be wrapped, and this allows us to specify different optimizers for testing.  the next step after the below code would be to determine a way to wrap both classes and grid search the parameters continuously"],"metadata":{}},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, RegressorMixin\nfrom keras.layers import LSTM\nclass Sequential_Wrapper(BaseEstimator,RegressorMixin,):\n  def __init__(self,loss = 'mae', activation= 'relu', optimizer = 'adam'):\n    self.model = Sequential()\n    self.loss = loss\n    self.activation = activation\n    self.optimizer = optimizer\n    return \n  \n  def fit(self,X,y=None):\n    from sklearn.preprocessing import MinMaxScaler\n    self.scl_X = MinMaxScaler(feature_range=(0, 1))\n    self.scl_y = MinMaxScaler(feature_range=(0, 1))\n    X_scl = self.scl_X.fit_transform(X)\n    y_scl = self.scl_y.fit_transform(y.reshape(-1,1))\n    X_scl_re = X_scl.reshape((X_scl.shape[0], 1, X_scl.shape[1]))\n    self.model.add(LSTM(75, \n                        input_shape=(X_scl_re.shape[1], X_scl_re.shape[2])))\n    self.model.add (Dropout(0.2))\n    self.model.add(Dense(1, activation = self.activation))\n    self.model.compile(loss=self.loss, optimizer= self.optimizer)\n    self.model.fit(X_scl_re, y_scl, \n                   epochs    =50, \n                   batch_size=50, \n                   verbose   =0, \n                   shuffle   =False)\n    return self\n  \n  def predict(self,X,y=None):\n    X_scl    = self.scl_X.transform(X)\n    X_scl_re = X_scl.reshape((X_scl.shape[0], 1, X_scl.shape[1]))\n    return self.scl_y.inverse_transform(self.model.predict(X_scl_re))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":52},{"cell_type":"code","source":["from spark_sklearn           import GridSearchCV\nfrom sklearn.pipeline        import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics         import make_scorer, mean_absolute_error, r2_score\nsimple_gs_seq = \\\nGridSearchCV(sc,\n             estimator=Pipeline(steps=[('seq',Sequential_Wrapper())\n                                      ]),\n             param_grid={'seq__loss':['mae', 'hinge'],\n                         'seq__activation':['relu', 'softmax'],\n                         'seq__optimizer': ['adam', 'SGD']\n                        },\n             cv=TimeSeriesSplit(n_splits=2),\n             scoring=make_scorer(r2_score),\n             return_train_score=False,\n             n_jobs=-1 \n            ) \nsimple_gs_seq \\\n  .fit(trn_fea_pdf_3.values,\n       trn_tgt_ser_3.values)\ndisplay_pdf(est_grid_results_pdf(simple_gs_seq,\n                                 est_tag='seq'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mean_test_score</th><th>seq__activation</th><th>seq__loss</th><th>seq__optimizer</th><th>est_tag</th></tr></thead><tbody><tr><td>0.762642927317379</td><td>relu</td><td>mae</td><td>SGD</td><td>seq</td></tr><tr><td>0.6837643845472708</td><td>relu</td><td>mae</td><td>adam</td><td>seq</td></tr><tr><td>-23.390404114077892</td><td>softmax</td><td>mae</td><td>adam</td><td>seq</td></tr><tr><td>-23.390404114077892</td><td>softmax</td><td>mae</td><td>SGD</td><td>seq</td></tr><tr><td>-23.390404114077892</td><td>softmax</td><td>hinge</td><td>adam</td><td>seq</td></tr><tr><td>-23.390404114077892</td><td>softmax</td><td>hinge</td><td>SGD</td><td>seq</td></tr><tr><td>-251.73458535681522</td><td>relu</td><td>hinge</td><td>SGD</td><td>seq</td></tr><tr><td>-14298.673537970697</td><td>relu</td><td>hinge</td><td>adam</td><td>seq</td></tr></tbody></table></div>"]}}],"execution_count":53},{"cell_type":"markdown","source":["We see above that the best activation function is RELU, or Rectified Linear, the loss function is Mean Average Error, and the optimizer is Stochastic Gradient Descent.  We could run through more parameters to test and such, but there is not much else we can do with the class from here."],"metadata":{}},{"cell_type":"markdown","source":["## Summary"],"metadata":{}},{"cell_type":"markdown","source":["Above, we took the LSTM model discussed briefly in class and did an exploration on how we could make it work for different lags, different batch sizes, and finally different optimizers/losses/activators.  Our study revealed that predictions on data lagged 7 days can prove quite complex and as a result we should consider different learning rates and a higher dropout rate with the longer time frame."],"metadata":{}}],"metadata":{"name":"5. Investigations - LSTM","notebookId":1454461},"nbformat":4,"nbformat_minor":0}
