{"cells":[{"cell_type":"markdown","source":["# Exercises - Week 5 - Cross Validation - [Blackjack]"],"metadata":{}},{"cell_type":"markdown","source":["## References\n- [3.1. Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html) (Scikit-learn documentation)\n\nNext week:\n- https://scikit-learn.org/stable/modules/grid_search.html\n- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection\n- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n- https://scikit-learn.org/stable/modules/model_evaluation.html\n- https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\n- https://scikit-learn.org/stable/tutorial/statistical_inference/index.html\n\nLast week:\n- https://scikit-learn.org/stable/data_transforms.html\n- https://scikit-learn.org/stable/modules/preprocessing.html\n- https://scikit-learn.org/stable/modules/compose.html#combining-estimators\n- https://scikit-learn.org/stable/modules/compose.html#featureunion-composite-feature-spaces"],"metadata":{}},{"cell_type":"markdown","source":["## Contents\n1. Setup \n2. Data Lab notebooks\n1. Cross validation\n1. Pipelines with `FeatureUnion`"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Setup"],"metadata":{}},{"cell_type":"markdown","source":["Load libraries and display version numbers."],"metadata":{}},{"cell_type":"code","source":["import pandas  as pd\nimport numpy   as np\nimport sklearn as sk\nprint('sklearn',sk.__version__)\nprint('pandas ',pd.__version__)\nprint('numpy  ',np.__version__)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">sklearn 0.20.3\npandas  0.24.2\nnumpy   1.16.2\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["These version numbers may not be the most recent or correspond to the documentation you locate via Google."],"metadata":{}},{"cell_type":"markdown","source":["The `display_pdf` function displays a pandas dataframe using the databricks display function."],"metadata":{}},{"cell_type":"code","source":["def display_pdf(a_pdf):\n  display(spark.createDataFrame(a_pdf))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["## 2. Data Lab notebooks\n\nLast week:\n- [sklearn/Introduction](https://bentley.cloud.databricks.com/#notebook/210807) \n- [sklearn/Preprocessing](https://bentley.cloud.databricks.com/#notebook/404771)"],"metadata":{}},{"cell_type":"markdown","source":["## 3. Cross validation"],"metadata":{}},{"cell_type":"markdown","source":["Cross validation is an extension of the approach using train and test datasets to creating models that generalize well on new data. \n\nIn the following sections, I'll review the train-test approach, provide background for cross validation, and describe the method."],"metadata":{}},{"cell_type":"markdown","source":["### 3.1 Cross validation - Train-test review \n\n- [Train & test datasets](https://bentley.cloud.databricks.com/#notebook/958305) (data lab notebook)"],"metadata":{}},{"cell_type":"markdown","source":["Estimators are used in three steps:\n1. Create a model (by fitting a dataset to the estimator)\n1. Make predictions with that model\n1. Evaluate the model by comparing these predictions with actual values (scoring)\n\nIt's important to create models that make good predictions on unseen data. [more TBD]\n\nAs a first step, do not create predictions on the same dataset which was fit to the estimator to create the model.\nInstead, (randomly) split the initial dataset into two datasets (_train_ and _test_) and:\n- create the model by fitting the estimator to the _train dataset_ \n- create predictions on the _test dataset_ (unseen data)\n\nThese predictions can then be scored to evaluate the model. A few common scoring functions are:\n- root mean square error and R squared for regression models\n- accuracy and area under the curve for classification models\n\nTrain-test datasets (and their proper use) ensure that data used to fit a model is kept separate from (unseen) data use to evaluate that model.\n\nThere are still potential problems with data from the training process leaking into the evaluation of the test dataset. __Cross Validation__ is a method that attempts to remedy this problem, and is described in the next section."],"metadata":{}},{"cell_type":"markdown","source":["### 3.2 Cross validation - Introduction"],"metadata":{}},{"cell_type":"markdown","source":["__Reference__ \n- [Train & test datasets](https://bentley.cloud.databricks.com/#notebook/958305) (data lab notebook)\n- [3.1. Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html) (Scikit-learn documentation)"],"metadata":{}},{"cell_type":"markdown","source":["This section provides background to the cross validation method in light of the problem described above.\n\nEvery estimator has a set of _hyper-parameters_ that determine how the model is created when the estimator is fit to a dataset.\n\nIt is tempting to:\n1. Choose a set of hyper-parameters\n1. Create a model by fitting the train dataset to the estimator determined by these hyper-parameters\n1. Score predictions made by this model on the test dataset\n1. Evaluate the model (as determined by these hyper-parameters) based on its prediction score\n1. Return to step 1 and repeat this process until they have found the \"best\" model in terms of its evaluation on the test dataset\n\nThere are problems, often referred to as _overfitting_, with this process:\n- The final model has been created (customized) so that it scores well on the test dataset\n- The test dataset has been used multiple times to find the best hyper-parameters of the final model\n\nEither way you look at it, the test dataset is no longer new or unseen. Information from the training process has _leaked into_ the evaluation process. The Scikit-learn documentation calls this a \"methodolical error\"."],"metadata":{}},{"cell_type":"markdown","source":["We have two objectives: \n1. Use the test dataset only once to evaluate the final model\n1. Find the best hyper-parameters to use in creating the model (without using the test dataset)\n\nTo address the first objective, separate the initial dataset into a train dataset and a test dataset. \n\nTo address the second objective, cross validation separates the train dataset into multiple pairs of _train_ and _validation_ datasets. These train-validation dataset pairs are used to find the best hyper-parameters. The final model is then evaluated on the test dataset.\n\nThe following sections, provide more detail into:\n- the process of creating the train-validation dataset pairs\n- the use of cross validation to find the best hyper-parameters."],"metadata":{}},{"cell_type":"markdown","source":["### 3.3 Cross validation - Method"],"metadata":{}},{"cell_type":"markdown","source":["This section describes the cross validation method. In basic terms: \n- The input to the method is a single dataset, in particular it is the train dataset from the initial split of train and test datasets. \n- The output from the method consists of several train-validation dataset pairs. \n\nThere are two methods for creating these train-validation dataset pairs for cross-sectional and time series (respectively)."],"metadata":{}},{"cell_type":"markdown","source":["For cross-sectional datasets, the _KFold_ procedure (described below) is the most straightforward. See this [link](https://scikit-learn.org/stable/modules/cross_validation.html#k-fold) from Scikit-learn for a graphic that may help in understand the procedure.\n\nTrain-validation pairs are created as follows, given an input parameter `k`:\n1. The input dataset is randomly partitioned into `k` equally sized subsets\n2. One of the `k` subsets is designated as the validation dataset\n3. The train dataset paired with the validation dataset, created in step 2, consists of the remaining `k-1` subsets \n4. Repeat steps 2 and 3 for each of the `k` subsets. This produces `k` train-validation pairs. \n\nFor more information see [3.1. Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html) from the [Scikit-learn documentation](https://scikit-learn.org/stable/index.html)."],"metadata":{}},{"cell_type":"markdown","source":["On the other hand, the train dataset of a time series dataset must precede in time the validation/test dataset. This means that the standard/classical cross-validation method (as described above) will not work (isn't acceptable) for time series. See this [link](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split) from Scikit-learn that may help in understanding the procedure. \n\nFor time series datasets, the train-validation pairs are created as follows, given an input parameter `k`:\n1. The input dataset is partitioned into `k+1` subsets of equal size (if possible), where each is a continuous sequence of rows \n1. The first train dataset (of the first train-validation pair) is the __first__ of the `k+1` subsets. The corresponding validation dataset is the second of the subsets. \n1. The second train dataset is the __first two__ of the `k+1` subsets. The corresponding validation dataset is the third of the subsets. \n1. The last train dataset is the __first `k`__ of the `k+1` subsets. The corresponding validation dataset is the last of the subsets. \n\nAn example is presented in the next two code cells below to demonstrate the procedure. The first cell creates a dataset with 60 rows."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nX = \\\npd.DataFrame(data={'a':range(100,160),\n                   'b':range(200,260)})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["Run the code cell below with different values of `n_splits` in the first line of the cell. Numbers between `2` and `5` produce splits that are easy to understand."],"metadata":{}},{"cell_type":"code","source":["n_splits = 5\n\nfrom sklearn.model_selection import TimeSeriesSplit\ntscv = TimeSeriesSplit(n_splits=n_splits)\n\nfor train, test in tscv.split(X):\n  print(\"min(train)=%s, max(train)=%2d, min(test)=%s, max(test)=%s, len(train)=%s, len(test)=%s\" \n        % (min(train), max(train), min(test),  max(test), len(train), len(test)))\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">min(train)=0, max(train)= 9, min(test)=10, max(test)=19, len(train)=10, len(test)=10\nmin(train)=0, max(train)=19, min(test)=20, max(test)=29, len(train)=20, len(test)=10\nmin(train)=0, max(train)=29, min(test)=30, max(test)=39, len(train)=30, len(test)=10\nmin(train)=0, max(train)=39, min(test)=40, max(test)=49, len(train)=40, len(test)=10\nmin(train)=0, max(train)=49, min(test)=50, max(test)=59, len(train)=50, len(test)=10\n</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["For more information see\n[TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html),\n[3.1.2.5. Cross validation of time series data](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-of-time-series-data)\nfrom the \n[Scikit-learn documentation](https://scikit-learn.org/stable/index.html)."],"metadata":{}},{"cell_type":"markdown","source":["__Exercise:__ create a demonstration, as simple as the demonstration of `TimeSeriesSplit` above, of basic cross validation with a cross-sectional dataset using the `KFold` class and using a very simple dataset."],"metadata":{}},{"cell_type":"markdown","source":["In the code below, we define the number of splits to be 5, and then import the KFold object from sklearn.  By specifying the n_splits argument in our version of KFold (KF) we are changing this away from the default.  We then utilize a for loop to perform a split and subsequent cross validation of the data.  This performance allows us to create cross validated data and to split our data randomly, this will be important moving forward because the need to validate which methods and hyper-parameters are most useful will be done on cross validation, as a certain trained dataset may produce overfit results."],"metadata":{}},{"cell_type":"code","source":["n_splits = 5\nimport numpy as np\nfrom sklearn.model_selection import KFold\nKF= KFold(n_splits=n_splits)\n\nfor train, test in KF.split(X):\n  print(\"min(train)=%s, max(train)=%2d, min(test)=%s, max(test)=%s, len(train)=%s, len(test)=%s\" \n        % (min(train), max(train), min(test),  max(test), len(train), len(test)))\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">min(train)=12, max(train)=59, min(test)=0, max(test)=11, len(train)=48, len(test)=12\nmin(train)=0, max(train)=59, min(test)=12, max(test)=23, len(train)=48, len(test)=12\nmin(train)=0, max(train)=59, min(test)=24, max(test)=35, len(train)=48, len(test)=12\nmin(train)=0, max(train)=59, min(test)=36, max(test)=47, len(train)=48, len(test)=12\nmin(train)=0, max(train)=47, min(test)=48, max(test)=59, len(train)=48, len(test)=12\n</div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["## 4. Pipelines with `FeatureUnion`"],"metadata":{}},{"cell_type":"markdown","source":["`FeatureUnion` combines several transformer objects into a new transformer that combines their output. --- Scikit-learn \n\nThe `fit` and `transform` methods of a `FeatureUnion` object initiate the same methods on each component transformer object.\nThe result of the `transform` method (of the `FeatureUnion` object) is the column-wise concatenation of the results of the `transform` methods applied to the component transformer objects. \n\nFor example:"],"metadata":{}},{"cell_type":"code","source":["from sklearn.pipeline                import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\ncorpus = [\n  'dogs and cats.',\n  'dogs, more dogs and horses.',\n  'cats or birds.'\n]\nfea = FeatureUnion([('cnt_vec', CountVectorizer()),\n                    ('idf_vec', TfidfVectorizer())\n                   ])\nfea_pdf = \\\nfea.fit_transform(corpus) \\\n   .toarray() \\\n   .round(3)\ndisplay_pdf(pd.DataFrame(data=fea_pdf))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th></tr></thead><tbody><tr><td>1.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.577</td><td>0.0</td><td>0.577</td><td>0.577</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>1.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.344</td><td>0.0</td><td>0.0</td><td>0.688</td><td>0.452</td><td>0.452</td><td>0.0</td></tr><tr><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.623</td><td>0.474</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.623</td></tr></tbody></table></div>"]}}],"execution_count":32},{"cell_type":"code","source":["import pandas as pd \ntitle_tags_pdf = \\\npd.read_csv('/dbfs/mnt/group-ma707/data/mining_com_coal.csv',\n            encoding=\"ISO-8859-1\"\n           ) \\\n  .loc[:,['title','tags']\n       ] \\\n  .assign(target = lambda df: 1*df.tags.str.contains('china').astype('bool')\n         ) \\\n  .loc[lambda df: df.title.notnull()]\ntype(title_tags_pdf)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">16</span><span class=\"ansired\">]: </span>pandas.core.frame.DataFrame\n</div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["__Exercise:__ describe the function of each of the method calls above and describe how they work"],"metadata":{}},{"cell_type":"markdown","source":["The functionality of the code above works as follows:\n  the first line simply serves to read in the CSV file specified and uses the appropriate ISO coding methodology.\n  The first. loc function locates all rows from the columns titled \"title\" and \"tags\".\n  The next function (. assign) implements a lambda function which is implemented to show that if a column has the value china, it will be returned as Boolean type true.  By multiplying by 1, the value returned will be either 1 (for true) or 0 (for false).  This allows us to create a sparse array to be used in the upcoming exercises.  The next. loc function again utilizes the lambda function to create a function which locates within the data frame specified places where the title is not null."],"metadata":{}},{"cell_type":"markdown","source":["__Exercise:__ From the `title_tags_pdf` dataframe create:\n- a `features_pdf` dataframe containing the `title` column\n- a `target_ser` series containing the `target` column"],"metadata":{}},{"cell_type":"markdown","source":["The below code creates the necessary dataframes, there is not much to mention within this exact code segment as interesting."],"metadata":{}},{"cell_type":"code","source":["features_pdf=title_tags_pdf['title']\nprint(type(features_pdf))\ntarget_ser=title_tags_pdf.target\nprint(type(target_ser))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &apos;pandas.core.series.Series&apos;&gt;\n&lt;class &apos;pandas.core.series.Series&apos;&gt;\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\na=log_reg.fit(TfidfVectorizer().fit_transform(title_tags_pdf['title']),\n            title_tags_pdf.target\n           )\nlog_reg.predict(TfidfVectorizer().fit_transform(title_tags_pdf['title']))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">59</span><span class=\"ansired\">]: </span>array([0, 0, 1, ..., 0, 0, 0])\n</div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["__Exercise:__ Create a pipeline `est` containing `TfidfVectorizer` and `LogisticRegression` so that you can call\n- `est.fit(features_pdf,target_ser)`\n- `est.predict(features_pdf)`"],"metadata":{}},{"cell_type":"markdown","source":["The estimator pipeline below takes its first argument as the TfidfVectorizer (used to fit data) and then the Logistic Regression for its second argument (used to predict) and is able to produce results that print out in an array.  This is a simple example of how to create an estimator pipeline that utilizes different methods - and it is important to note that we keep the Logistic Regression method last, as it is the only one that functions with a predict."],"metadata":{}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model  import LogisticRegression\nfrom sklearn.preprocessing   import MinMaxScaler\nfrom sklearn.impute          import SimpleImputer\n\nest = Pipeline ([\n ('tfid', TfidfVectorizer()),\n ('lgr', LogisticRegression())\n])\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":42},{"cell_type":"code","source":["est.fit(features_pdf,target_ser)\nest.predict(features_pdf)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">66</span><span class=\"ansired\">]: </span>array([0, 0, 1, ..., 0, 0, 0])\n</div>"]}}],"execution_count":43},{"cell_type":"markdown","source":["## 5. Dataset"],"metadata":{}},{"cell_type":"markdown","source":["Display the paths of the three files in our dataset."],"metadata":{}},{"cell_type":"code","source":["%sh ls -hot /dbfs/mnt/group-ma707/data/*"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">-rw-r--r-- 1 root 259K Jan 29 17:44 /dbfs/mnt/group-ma707/data/5tc_plus_ind_vars.csv\n-rw-r--r-- 1 root  12M Jan 29 17:44 /dbfs/mnt/group-ma707/data/mining_com_coal.csv\n-rw-r--r-- 1 root  11M Jan 29 17:44 /dbfs/mnt/group-ma707/data/mining_com_iron_ore.csv\n</div>"]}}],"execution_count":46},{"cell_type":"markdown","source":["__Note:__ we will create, from the above files, at least three dataframes (with features and target). Each is described below.\n\nFrom only the 5TC dataset: \n- target will be `BCI`\n- features will include lagged versions of the other columns \n- features will include date and time components (hour, day of week, etc.)\n- features may include external time series\n\nFrom only the _mining_ dataset(s):\n- the target may be one or more tags (from the `tags` variable)\n- features would be words present in the `content` or `title` variables\n\nFrom the 5TC and _mining_ dataset(s): \n- target will be `BCI` (from 5TC dataframe)\n- include all features from either of the above dataframes\n- the dataframes would need to be joined by either:\n    1. aggregating the features from the _mining_ dataframe (by date)\n    1. spreading the 5TC dataframe onto the _mining_ dataframe (duplicating 5TC rows)"],"metadata":{}},{"cell_type":"markdown","source":["__The End__"],"metadata":{}}],"metadata":{"name":"W05.1 Ex. Cross validation (1)","notebookId":1086066},"nbformat":4,"nbformat_minor":0}
