{"cells":[{"cell_type":"markdown","source":["# Exercises - Week 4 - Feature Union - Blackjack"],"metadata":{}},{"cell_type":"markdown","source":["## References\n- https://scikit-learn.org/stable/data_transforms.html\n- https://scikit-learn.org/stable/modules/preprocessing.html\n- https://scikit-learn.org/stable/modules/compose.html#combining-estimators\n- https://scikit-learn.org/stable/modules/compose.html#featureunion-composite-feature-spaces"],"metadata":{}},{"cell_type":"markdown","source":["## Contents\n1. Setup \n2. Data Lab notebooks\n3. Dataset\n4. Wrapper transformer classes\n5. Types of pipelines\n6. `FeatureUnion`"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Setup"],"metadata":{}},{"cell_type":"markdown","source":["Load libraries and display version numbers."],"metadata":{}},{"cell_type":"code","source":["import pandas  as pd\nimport numpy   as np\nimport sklearn as sk\nprint('sklearn',sk.__version__)\nprint('pandas ',pd.__version__)\nprint('numpy  ',np.__version__)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">sklearn 0.20.3\npandas  0.24.2\nnumpy   1.16.2\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["These version numbers may not be the most recent or correspond to the documentation you locate via Google."],"metadata":{}},{"cell_type":"markdown","source":["The `display_pdf` function displays a pandas dataframe using the databricks display function."],"metadata":{}},{"cell_type":"code","source":["def display_pdf(a_pdf):\n  display(spark.createDataFrame(a_pdf))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["## 2. Data Lab notebooks\n- [sklearn/Introduction](https://bentley.cloud.databricks.com/#notebook/210807) \n- [sklearn/Preprocessing](https://bentley.cloud.databricks.com/#notebook/404771)\n- [Variance Thresholding For Feature Selection](https://bentley.cloud.databricks.com/#notebook/434422)\n- [Univariate Feature Selection](https://bentley.cloud.databricks.com/#notebook/478847)"],"metadata":{}},{"cell_type":"markdown","source":["## 3. Dataset"],"metadata":{}},{"cell_type":"markdown","source":["Display the paths of the three files in our dataset."],"metadata":{}},{"cell_type":"code","source":["%sh ls -hot /dbfs/mnt/group-ma707/data/*"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">-rw-r--r-- 1 root 259K Jan 29 17:44 /dbfs/mnt/group-ma707/data/5tc_plus_ind_vars.csv\n-rw-r--r-- 1 root  12M Jan 29 17:44 /dbfs/mnt/group-ma707/data/mining_com_coal.csv\n-rw-r--r-- 1 root  11M Jan 29 17:44 /dbfs/mnt/group-ma707/data/mining_com_iron_ore.csv\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["__Exercise:__ write functions `get_coal_pdf`, `get_iron_ore_pdf` and `get_bci_pdf` which \n- read dataframes from the respective CSV files\n- replace the tilde with an underscore (in the variable name with the tilde)\n- rename the variables to snake case (lowercase and underscores between words)\n- create a variable `timestamp` in the mining dataframes of type `datetime64`\n- create `date` variables of type `datetime64` with only year, month, and day (in all 3 functions)"],"metadata":{}},{"cell_type":"markdown","source":["The below function imports the pandas library to be used in creating dataframes.  The first line of the function serves to create the dataframe with the appropriate encoding for the dataframe.  The next line creates the snake case column names, and finally the final column creates a timestamp date in the appropriate format, thus eliminating the extraneous data from the iron ore and coal data in the form of timestamps."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\ndef get_coal_pdf():\n  x = pd.read_csv('/dbfs/mnt/group-ma707/data/mining_com_coal.csv', encoding = \"ISO-8859-1\")\n  x.columns = x.columns.str.lower().str.replace(' ','_').str.replace('-', '_')\n  mining_com_coal_pdf['timestamp'] = pd.to_datetime(mining_com_coal_pdf['date'],format='%m/%d/%Y')\n \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["Like the function above, the one below follows the same basic principle.  This one has the added feature of creating a new date from the dataframe which serves to create the needed date data from the iron ore file."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\ndef get_iron_ore_pdf():\n  mining_com_iron_ore_pdf = pd.read_csv ('/dbfs/mnt/group-ma707/data/mining_com_iron_ore.csv', encoding = \"ISO-8859-1\")\n  mining_com_iron_ore_pdf['timestamp'] = pd.to_datetime(mining_com_iron_ore_pdf['date'],format='%Y-%m-%d %H:%M:%S')\n  mining_com_iron_ore_pdf['date2']=mining_com_iron_ore_pdf['date'].str.slice(0,10)\n  mining_com_iron_ore_pdf['date_new'] = pd.to_datetime(mining_com_iron_ore_pdf['date2'],format='%Y-%m-%d')\n  \n  return mining_com_iron_ore_pdf\nmining_com_iron_ore_pdf= get_iron_ore_pdf()\n#mining_com_iron_ore_pdf.head()\n\nmining_com_coal_pdf=pd.read_csv('/dbfs/mnt/group-ma707/data/mining_com_coal.csv', encoding = \"ISO-8859-1\")\nmining_com_coal_pdf.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">47</span><span class=\"ansired\">]: </span>\n                                             content  ...                                                url\n0  Electric cars are better for the environment t...  ...  http://www.mining.com/web/electric-cars-seen-g...\n1  Project Halo, the South African consortium sel...  ...  http://www.mining.com/project-halo-rejects-fav...\n2  * China&apos;s 2018 iron ore imports fall to 1.064 ...  ...  http://www.mining.com/web/chinas-2018-iron-ore...\n3  Angela Merkel this week will seek to bridge di...  ...  http://www.mining.com/web/merkel-seeks-heal-ri...\n4  A coal mine roof collapse in northwest China&apos;s...  ...  http://www.mining.com/web/coal-mine-collapse-c...\n\n[5 rows x 7 columns]\n</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["Note the differences here between the data frame creation (no ISD encoding needed) and the lack of the new date because the target bci5 variable not having a time portion to the date function."],"metadata":{}},{"cell_type":"code","source":["def bci_5tc_plus_pdf():\n  bci_5tc_plus_pdf = pd.read_csv('/dbfs/mnt/group-ma707/data/5tc_plus_ind_vars.csv')\n  bci_5tc_plus_pdf['Date_new']=pd.to_datetime(bci_5tc_plus_pdf['Date'],format='%Y-%m-%d')\n  return bci_5tc_plus_pdf\nbci_5tc_plus_pdf=bci_5tc_plus_pdf()\n#bci_5tc_plus_pdf.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["__Exercise:__ demonstrate that there are multiple entries for some dates (year, month, day) in the mining dataframes, using: \n- the `to_datetime` function from pandas\n- the `dt` and `date` attributes\n- the `value_counts` and `sort_values` methods \n\nDo so with a single method chain (hence no variables)."],"metadata":{}},{"cell_type":"markdown","source":["By using the value counts and describe functionality incorporated we can see (output) that there are multiple points at which certain dates occur.  this will create problems later on in our analyses as we have to handle a target variable with only one date, but features that have mutliple entries on the same date."],"metadata":{}},{"cell_type":"code","source":["a= mining_com_iron_ore_pdf['date_new'].value_counts()\nmining_com_iron_ore_pdf['date_new'].describe()\na"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">49</span><span class=\"ansired\">]: </span>\n2012-08-08    9\n2012-10-05    9\n2012-10-31    8\n2012-11-20    7\n2012-09-14    7\n2011-06-21    7\n2016-10-25    6\n2014-03-10    6\n2012-07-25    6\n2013-12-03    6\n2013-07-19    6\n2016-01-08    6\n2012-09-28    6\n2015-12-07    6\n2016-01-28    6\n2018-10-29    6\n2014-09-24    6\n2013-10-16    6\n2016-11-23    5\n2014-11-13    5\n2013-05-09    5\n2014-10-16    5\n2012-10-03    5\n2016-04-22    5\n2012-09-13    5\n2016-05-03    5\n2016-11-29    5\n2012-04-18    5\n2011-07-06    5\n2012-09-18    5\n             ..\n2016-05-06    1\n2017-04-09    1\n2018-05-31    1\n2012-03-06    1\n2014-03-20    1\n2014-04-23    1\n2018-11-16    1\n2010-03-16    1\n2013-11-10    1\n2016-05-17    1\n2013-07-30    1\n2013-02-15    1\n2009-11-24    1\n2012-03-07    1\n2016-09-30    1\n2017-11-21    1\n2013-01-18    1\n2010-05-12    1\n2016-01-26    1\n2017-01-16    1\n2008-11-22    1\n2012-06-05    1\n2014-09-17    1\n2011-12-15    1\n2014-09-01    1\n2008-04-28    1\n2015-03-05    1\n2018-08-07    1\n2017-12-02    1\n2011-12-27    1\nName: date_new, Length: 1996, dtype: int64\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["__Exercise:__ demonstrate that there are no multiple entries for any dates in the 5TC dataframe, using: \n- the `to_datetime` function from pandas\n- the `dt` and `date` attributes\n- the `value_counts` and `describe` methods \n\nDo so with a single method chain (hence no variables)."],"metadata":{}},{"cell_type":"markdown","source":["Using the same functionality as displayed above, we can see that the count and unique functions are exactly the same, indicating that there are exactly 1602 date variables and all of them are unique in the 5TC dataframe."],"metadata":{}},{"cell_type":"code","source":["bci_5tc_plus_pdf['Date_new'].value_counts().describe()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">50</span><span class=\"ansired\">]: </span>\ncount                    1602\nunique                   1602\ntop       2018-01-05 00:00:00\nfreq                        1\nfirst     2011-12-05 00:00:00\nlast      2019-01-23 00:00:00\nName: Date_new, dtype: object\n</div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["__Note:__ we will create, from the above files, at least three dataframes (with features and target). Each is described below.\n\nFrom only the 5TC dataset: \n- target will be `BCI`\n- features will include lagged versions of the other columns \n- features will include date and time components (hour, day of week, etc.)\n- features may include external time series\n\nFrom only the _mining_ dataset(s):\n- the target may be one or more tags (from the `tags` variable)\n- features would be words present in the `content` or `title` variables\n\nFrom the 5TC and _mining_ dataset(s): \n- target will be `BCI` (from 5TC dataframe)\n- include all features from either of the above dataframes\n- the dataframes would need to be joined by either:\n    1. aggregating the features from the _mining_ dataframe (by date)\n    1. spreading the 5TC dataframe onto the _mining_ dataframe (duplicating 5TC rows)"],"metadata":{}},{"cell_type":"markdown","source":["__Exercise:__ discuss with your group which of the two options for the third (combined) dataset look more promising or most reasonable."],"metadata":{}},{"cell_type":"markdown","source":["After discussing at length, we believe that aggregating the features from the mining data frame will provide the better answer.  Our reasoning is as follows - as the 5TC variable is our target variable, we do not want multiple instances of the same target.  By aggregating the data from the mining data frames, we are better providing ourselves with an accurate representation of all things impacting the target variable on the given day in which it was measured.  Furthermore, we believe it will be important to see and understand if and when the target variable was collected, and perhaps segregate data in a way that is more akin to its collection than the standard 24 hour clock (i.e. for our analysis, the day either starts or ends with the collection of the target value, and all features are collected before or after)."],"metadata":{}},{"cell_type":"markdown","source":["## 4. Wrapper transformer classes"],"metadata":{}},{"cell_type":"markdown","source":["__Note:__ Below is a template for a transformer class."],"metadata":{}},{"cell_type":"code","source":["class TransformerExample(BaseEstimator, TransformerMixin):\n  def __init__(self, init_param1, init_param2):\n    self.init_param1 = init_param1 \n    self.init_param2 = init_param2    \n    self.vec = CountVectorizer()\n  def fit(self, X, y=None):\n    # set attributes of self and then return self\n    return self\n  def transform(self, X):\n    # return a dataframe/array from X\n    # do not modify X\n    my_df = pd.DataFrame(vec.fit_transform(X))\n    my_df.columns = self.init_param1\n    return "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["__Exercise:__ create wrapper transformer classes called `CountVectorizerDF` and `TfidfVectorizerDF` which call their counterparts above (`CountVectorizer` and `TfidfVectorizer`) and return a pandas dataframe with:\n- column names from the `get_feature_names` method (of the counterpart object)\n- values from the `fit_transform` method (of the counterpart object)\n\nYou may need to use the `toarray` method on the `fit_transform` output. Demonstrate these two classes using `corpus` (below)."],"metadata":{}},{"cell_type":"markdown","source":["The code below shows a wrapper transformer class for CountVectorizer.  We begin by defining the init method to contain only the CountVectorizer method.  We then use the fit_transform method on the data that is passed and obtain the column names by using \"get_feature_names\" functionality of CountVectorizer.  Finally, in the transform method we create a data frame from what was determined above and return a pandas data frame."],"metadata":{}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["The following code takes the CountVectorizer class and wraps it in our own function.  Note the specific use of the \"super\" method to appoint portions of the wrapped method to our method.  The super method calls the parent class on the wrapper class and applies the selected portions of the super class onto it - for example, the fit method of CountVectorizer is applied verbatim to the fit method in our created class.  The main difference from our created class and the CountVectorizer class is the return of a Data Frame."],"metadata":{}},{"cell_type":"code","source":["class CountVectorizerDF2(CountVectorizer):\n  def __init__(self):\n    super().__init__()\n  def fit(self,X, y = None):\n    super().fit(X,y)\n    self.columns = super().fit(X,y).get_feature_names()\n    return self\n  def transform(self,X):\n    abc = pd.DataFrame(super().transform(X).toarray())\n    abc.columns = self.columns\n    return abc"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["The \"corpus\" code is simply used as tester for our created methods."],"metadata":{}},{"cell_type":"code","source":["corpus = [\n  'dogs and cats.',\n  'dogs, more dogs and horses.',\n  'cats or birds.']"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":40},{"cell_type":"markdown","source":["Results of the Corpus expirement on the CountVectorizer wrapper class.  Note that the code works as expected, however we are unable to use the fit_transform method as we have not defined it in our class.  Because we called a super class and only implemented parts, we will have to independenlty call the fit_transfrom method and define it in our class."],"metadata":{}},{"cell_type":"code","source":["CountVectorizerDF2().fit(corpus).transform(corpus)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">57</span><span class=\"ansired\">]: </span>\n   and  birds  cats  dogs  horses  more  or\n0    1      0     1     1       0     0   0\n1    1      0     0     2       1     1   0\n2    0      1     1     0       0     0   1\n</div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["Our results shown above match the results generated earlier."],"metadata":{}},{"cell_type":"markdown","source":["Like above, we begin to specify the class, and define our init method to include only self.  Rather than fully re-explaining the methodology, we note that the steps taken for TfidfVectorizer do not differ from those for countVectorizer, it simply utilizes Tfidf as opposed to CountVectorizer."],"metadata":{}},{"cell_type":"code","source":["class TfidfVectorizerDF2(TfidfVectorizer):\n  def __init__(self):\n    super().__init__()\n  def fit(self, X, y = None):\n    super().fit(X, y)\n    self.columns = super().fit(X,y).get_feature_names()\n    return self\n  def transform(self, X):\n    abc = pd.DataFrame(super().transform(X).toarray(), columns  = self.columns)\n    return abc"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":45},{"cell_type":"markdown","source":["We see here the output from the Tfidf method.  We note that it is different because of the differnt computing methods between Tfidf and CountVec."],"metadata":{}},{"cell_type":"code","source":["result=TfidfVectorizerDF2().fit(corpus).transform(corpus)\nresult"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">64</span><span class=\"ansired\">]: </span>\n        and     birds     cats      dogs    horses      more        or\n0  0.577350  0.000000  0.57735  0.577350  0.000000  0.000000  0.000000\n1  0.343851  0.000000  0.00000  0.687703  0.452123  0.452123  0.000000\n2  0.000000  0.622766  0.47363  0.000000  0.000000  0.000000  0.622766\n</div>"]}}],"execution_count":47},{"cell_type":"markdown","source":["__Exercise:__ create a wrapper transformer class called `CountVectColDF`.\n1. The init method should have a parameter called `col_name`\n1. The init method creates and then stores a `CountVectorizer` object in an attribute called `vec`\n1. The `transform` method expects a pandas dataframe in the `X` argument\n1. The `transform` method passes the `col_name` column of the `X` dataframe to the `transform` method of the `CountVectorizerDF` object stored in `vec`\n1. The `transform` method then returns the result of the previous item (4).\n\nTest your class with the command `CountVectColDF('tags').fit_transform(mining_com_coal_pdf)`"],"metadata":{}},{"cell_type":"markdown","source":["The following code takes the CountVectorizerDF2 class and wraps it in a new function caleld CountVectColDF2. We again use the super methodology to apply the column name of a specified dataframe and then employ a count vectorizer wrapped class to the argument.  Note that this method only works based on our method above working, which is why we cannot use fit_transform and instead use fit().transform() chained together.  We take the argument entered in 'tags' and pull the tags column from the specified data frame.  This will be useful in performing our analysis later on as we will be able to pull out the needed coolumns and run a count vectorizer or tfdif type analysis on them."],"metadata":{}},{"cell_type":"code","source":["class CountVectColDF2(CountVectorizerDF2):\n  def __init__ (self, col_name):\n    super().__init__()\n    self.col_name = col_name\n  def fit (self, X, y = None):\n    self.doc=np.array(X[self.col_name]).astype('U').tolist()\n    super().fit(self.doc,y)\n    return self\n  def transform (self, X):\n    abc = pd.DataFrame(super().transform(self.doc))\n    return abc"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":50},{"cell_type":"code","source":["result=CountVectColDF2('tags').fit(mining_com_coal_pdf).transform(mining_com_coal_pdf)\nresult.info()\nprint(result)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;\nRangeIndex: 4460 entries, 0 to 4459\nColumns: 836 entries, 2012 to zinc\ndtypes: int64(836)\nmemory usage: 28.4 MB\n      2012  abb  aca  accident  acid  ...  zealand  zijin  zimbabwe  zimplats  zinc\n0        0    0    0         0     0  ...        0      0         0         0     0\n1        0    0    0         0     0  ...        0      0         0         0     0\n2        0    0    0         0     0  ...        0      0         0         0     0\n3        0    0    0         0     0  ...        0      0         0         0     0\n4        0    0    0         0     0  ...        0      0         0         0     0\n5        0    0    0         0     0  ...        0      0         0         0     0\n6        0    0    0         0     0  ...        0      0         0         0     0\n7        0    0    0         0     0  ...        0      0         0         0     0\n8        0    0    0         0     0  ...        0      0         0         0     1\n9        0    0    0         0     0  ...        0      0         0         0     0\n10       0    0    0         0     0  ...        0      0         0         0     0\n11       0    0    0         0     0  ...        0      0         0         0     0\n12       0    0    0         0     0  ...        0      0         0         0     0\n13       0    0    0         0     0  ...        0      0         0         0     0\n14       0    0    0         0     0  ...        0      0         0         0     0\n15       0    0    0         0     0  ...        0      0         0         0     0\n16       0    0    0         0     0  ...        0      0         0         0     0\n17       0    0    0         0     0  ...        0      0         0         0     1\n18       0    0    0         0     0  ...        0      0         0         0     0\n19       0    0    0         0     0  ...        0      0         0         0     0\n20       0    0    0         0     0  ...        0      0         0         0     0\n21       0    0    0         0     0  ...        0      0         0         0     0\n22       0    0    0         0     0  ...        0      0         0         0     0\n23       0    0    0         0     0  ...        0      0         0         0     0\n24       0    0    0         0     0  ...        0      0         0         0     0\n25       0    0    0         0     0  ...        0      0         0         0     0\n26       0    0    0         0     0  ...        0      0         0         0     0\n27       0    0    0         0     0  ...        0      0         0         0     0\n28       0    0    0         0     0  ...        0      0         0         0     0\n29       0    0    0         0     0  ...        0      0         0         0     0\n...    ...  ...  ...       ...   ...  ...      ...    ...       ...       ...   ...\n4430     0    0    0         0     0  ...        0      0         0         0     0\n4431     0    0    0         0     0  ...        0      0         0         0     0\n4432     0    0    0         0     0  ...        0      0         0         0     0\n4433     0    0    0         0     0  ...        0      0         0         0     0\n4434     0    0    0         0     0  ...        0      0         0         0     0\n4435     0    0    0         0     0  ...        0      0         0         0     0\n4436     0    0    0         0     0  ...        0      0         0         0     0\n4437     0    0    0         0     0  ...        0      0         0         0     0\n4438     0    0    0         0     0  ...        0      0         0         0     0\n4439     0    0    0         0     0  ...        0      0         0         0     0\n4440     0    0    0         0     0  ...        0      0         0         0     0\n4441     0    0    0         0     0  ...        0      0         0         0     0\n4442     0    0    0         0     0  ...        0      0         0         0     0\n4443     0    0    0         0     0  ...        0      0         0         0     0\n4444     0    0    0         0     0  ...        0      0         0         0     0\n4445     0    0    0         0     1  ...        0      0         0         0     0\n4446     0    0    0         0     0  ...        0      0         0         0     0\n4447     0    0    0         0     0  ...        0      0         0         0     0\n4448     0    0    0         0     0  ...        0      0         0         0     0\n4449     0    0    0         0     0  ...        0      0         0         0     0\n4450     0    0    0         0     0  ...        0      0         0         0     0\n4451     0    0    0         0     0  ...        0      0         0         0     0\n4452     0    0    0         0     0  ...        0      0         0         0     0\n4453     0    0    0         0     0  ...        0      0         0         0     0\n4454     0    0    0         0     0  ...        0      0         0         0     0\n4455     0    0    0         0     0  ...        0      0         0         0     0\n4456     0    0    0         0     0  ...        0      0         0         0     0\n4457     0    0    0         0     0  ...        0      0         0         0     0\n4458     0    0    0         0     0  ...        0      0         0         0     0\n4459     0    0    0         0     0  ...        0      0         0         0     0\n\n[4460 rows x 836 columns]\n</div>"]}}],"execution_count":51},{"cell_type":"markdown","source":["## 5. Types of pipelines"],"metadata":{}},{"cell_type":"markdown","source":["There are two key observations about pipelines in `sklearn` that determine how I use them:\n1. The `transform` method does not transform the target variable (`y`)\n1. Transformer steps in an estimator pipeline cannot add or drop rows \n\nThis implies two types of pipelines, transformer pipelines and estimator pipelines, and suggests how to use them."],"metadata":{}},{"cell_type":"markdown","source":["Transformer pipelines: \n- create features for the estimator pipeline \n- includes, but doesn't modify, target variable values\n- fills in missing values of feature variables\n- may drop rows to remove rows with missing values \n- result in feature variables and a target variable with no missing values\n\nKeep transformations steps to a minimum for two reasons:\n1. there is no distinction between train and test data \n2. these pipelines cannot be part of grid search (as they aren't estimators)"],"metadata":{}},{"cell_type":"markdown","source":["Estimator pipelines: \n- should not contain missing values\n- include transformations that modify columns\n- can be run using grid search"],"metadata":{}},{"cell_type":"markdown","source":["Initially, we focus on creating classes for a transformer pipeline. Two of these classes are `DateVarsDF` and `LagVarsDF`, to be built in exercises below."],"metadata":{}},{"cell_type":"markdown","source":["__Exercise:__ create a transformer called `DatetimeVarsDF` that: \n- has an init method with one parameter `dt_col_name` that is the name of a `datetime` column (that is stored as an attribute of self)\n- has a `transform` method that returns a dataframe of all individual date and time variables derived from the datetime variable named in `dt_col_name`"],"metadata":{}},{"cell_type":"markdown","source":["The following code considers the DateTimeVarsDF that works as a transformer pipeline.  By utilizing the dt_col_name to be set to the col name specified by user input, we can take the resulting input and run an analysis that returns a pandas data frame showing the specific date and time from each observation in the given data.  This can prove useful for observing different dates that have multiple times associated and can be considered when performing the analysis discussed previously in merging target data frames with feature data frames."],"metadata":{}},{"cell_type":"code","source":["class  DatetimeVarsDF():\n  def __init__(self,dt_col_name):\n    self.dt_col_name = dt_col_name\n  def transform(self,df):\n    self.df=df\n    results=pd.to_datetime(self.df[self.dt_col_name],format='%Y-%m-%d %H:%M:%S')\n    return results\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":59},{"cell_type":"code","source":["a=DatetimeVarsDF('date').transform(mining_com_coal_pdf)\na"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">88</span><span class=\"ansired\">]: </span>\n0      2019-01-15 23:04:53+00:00\n1      2019-01-15 18:15:12+00:00\n2      2019-01-14 23:13:37+00:00\n3      2019-01-13 17:46:20+00:00\n4      2019-01-13 17:24:30+00:00\n5      2019-01-11 17:31:35+00:00\n6      2019-01-04 17:06:09+00:00\n7      2018-12-28 20:40:26+00:00\n8      2018-12-27 17:59:39+00:00\n9      2018-12-27 17:29:36+00:00\n10     2018-12-26 23:00:41+00:00\n11     2018-12-26 22:28:19+00:00\n12     2018-12-24 18:33:26+00:00\n13     2018-12-21 22:33:18+00:00\n14     2018-12-21 17:49:16+00:00\n15     2018-12-21 11:00:19+00:00\n16     2018-12-18 23:04:18+00:00\n17     2018-12-18 19:58:05+00:00\n18     2018-12-18 13:30:27+00:00\n19     2018-12-18 10:44:15+00:00\n20     2018-12-17 15:15:35+00:00\n21     2018-12-17 14:15:02+00:00\n22     2018-12-16 20:06:26+00:00\n23     2018-12-14 20:25:57+00:00\n24     2018-12-14 19:51:29+00:00\n25     2018-12-14 17:25:33+00:00\n26     2018-12-13 21:39:22+00:00\n27     2018-12-13 18:09:26+00:00\n28     2018-12-13 17:07:35+00:00\n29     2018-12-12 19:50:44+00:00\n                  ...           \n4430   2008-05-21 13:08:11+00:00\n4431   2008-05-21 09:45:37+00:00\n4432   2008-05-16 18:51:05+00:00\n4433   2008-05-15 22:38:06+00:00\n4434   2008-05-13 23:04:31+00:00\n4435   2008-05-13 13:02:17+00:00\n4436   2008-05-12 13:40:58+00:00\n4437   2008-05-09 20:41:57+00:00\n4438   2008-05-08 09:36:08+00:00\n4439   2008-05-06 15:51:01+00:00\n4440   2008-05-06 14:44:17+00:00\n4441   2008-05-01 15:56:17+00:00\n4442   2008-04-30 13:15:29+00:00\n4443   2008-04-30 03:57:09+00:00\n4444   2008-04-30 03:57:09+00:00\n4445   2008-04-28 22:12:40+00:00\n4446   2008-04-28 20:17:55+00:00\n4447   2008-04-28 20:14:14+00:00\n4448   2008-04-25 13:31:01+00:00\n4449   2008-04-25 13:26:38+00:00\n4450   2008-04-21 23:31:31+00:00\n4451   2008-04-21 22:02:13+00:00\n4452   2008-04-18 00:47:10+00:00\n4453   2008-04-17 20:57:34+00:00\n4454   2008-04-16 00:27:14+00:00\n4455   2008-04-15 13:41:54+00:00\n4456   2008-04-11 16:39:29+00:00\n4457   2008-04-10 00:56:26+00:00\n4458   2008-04-07 05:20:40+00:00\n4459   2008-04-01 07:11:28+00:00\nName: date, Length: 4460, dtype: datetime64[ns, UTC]\n</div>"]}}],"execution_count":60},{"cell_type":"markdown","source":["## 6. `FeatureUnion`"],"metadata":{}},{"cell_type":"markdown","source":["`FeatureUnion` combines several transformer objects into a new transformer that combines their output. --- Scikit-learn \n\nThe `fit` and `transform` methods of a `FeatureUnion` object initiate the same methods on each component transformer object.\nThe result of the `transform` method (of the `FeatureUnion` object) is the column-wise concatenation of the results of the `transform` methods applied to the component transformer objects. \n\nFor example:"],"metadata":{}},{"cell_type":"code","source":["from sklearn.pipeline                import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\ncorpus = [\n  'dogs and cats.',\n  'dogs, more dogs and horses.',\n  'cats or birds.'\n]\nfea = FeatureUnion([('cnt_vec', CountVectorizer()),\n                ('idf_vec', TfidfVectorizer())\n                   ])\nfea_pdf = \\\nfea.fit_transform(corpus) \\\n   .toarray() \\\n   .round(3)\ndisplay_pdf(pd.DataFrame(data=fea_pdf))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th></tr></thead><tbody><tr><td>1.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.577</td><td>0.0</td><td>0.577</td><td>0.577</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>1.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.344</td><td>0.0</td><td>0.0</td><td>0.688</td><td>0.452</td><td>0.452</td><td>0.0</td></tr><tr><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.623</td><td>0.474</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.623</td></tr></tbody></table></div>"]}}],"execution_count":63},{"cell_type":"markdown","source":["__Exercise:__ copy the `FeatureUnion` example above and modify it to use your two new transformer classes `CountVectorizerDF` and `TfidfVectorizerDF`."],"metadata":{}},{"cell_type":"markdown","source":["By modifying the FetureUnion above we are able to provide code that takes both methods, and then runs the same data through the code and concatenates the results.  This is useful for comparing side by sode results, and could be useful in the project anlaysis, but does not provide much in the way of \"new information\" and is more useful as a viewing tool. Here we note that CountVec rows are 0-6, and the tfdif are 7-13, indicating FetaureUnion merged the data side by side, we should consider this when applying a FeatureUnion command in our analysis."],"metadata":{}},{"cell_type":"code","source":["fea_Vec = FeatureUnion([('cnt_vec',CountVectorizerDF2()),\n                       ('idf_vec',TfidfVectorizerDF2())\n                      ])\nfea_pdf2 = fea_Vec.fit_transform(corpus).toarray().round(3) #Function(countvectorize) will return a dataframe, why fea_pdf2 is a ndarray\nprint(type(fea_pdf2))\ndisplay_pdf(pd.DataFrame(data=fea_pdf2))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th></tr></thead><tbody><tr><td>1.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.577</td><td>0.0</td><td>0.577</td><td>0.577</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>1.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.344</td><td>0.0</td><td>0.0</td><td>0.688</td><td>0.452</td><td>0.452</td><td>0.0</td></tr><tr><td>0.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.623</td><td>0.474</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.623</td></tr></tbody></table></div>"]}}],"execution_count":66},{"cell_type":"markdown","source":["__The End__"],"metadata":{}}],"metadata":{"name":"W04.1 Ex. Feature union","notebookId":1066980},"nbformat":4,"nbformat_minor":0}
