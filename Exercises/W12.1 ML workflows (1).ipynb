{"cells":[{"cell_type":"markdown","source":["# Machine Learning Workflows"],"metadata":{}},{"cell_type":"markdown","source":["## Contents\n1. Introduction\n2. Preprocessing pipelines\n3. Creating train-test dataset pairs\n4. Estimator pipelines"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Introduction"],"metadata":{}},{"cell_type":"markdown","source":["Machine learning workflows take raw data as input and produce models that make optimized predictions (as output).\nBy \"optimized predictions\" I mean predictions that minimize the difference (error) between the predicted and actual values. \n\nThese workflows are substantial and complicated processes. For this reason, they should be:\n- __modular__, which means that your code is both simple and capable\n- __standard__, which means that your code can be easily understood, debugged and extended by others\n\nPipelines satisfy both of these requirements and can be found in Python and Spark, among other languages."],"metadata":{}},{"cell_type":"markdown","source":["A machine learning workflow has three essential steps (the first and last are pipelines):\n1. A preprocessing pipeline, which creates a feature-target dataset suitable for fitting to the estimator pipeline(s)\n1. The separation of the feature-target dataset into train and test datasets \n1. An estimator pipeline, which can be used to create models and predictions from those models\n\nDescriptions of these steps are provided below."],"metadata":{}},{"cell_type":"markdown","source":["## 2. Preprocessing pipelines"],"metadata":{}},{"cell_type":"markdown","source":["Preprocessing pipelines are used to transform a raw dataset into a feature-target dataset suitable for:\n1. separating into a train-test dateset pair\n2. fitting the above train dataset to a `GridSearchCV` object initialized with an estimator pipeline and a parameter grid\n\nThe second step is often performed multiple times for different parameter grids. \n\nObjects in the pipeline may:\n- clean raw dataset features\n- create features from raw dataset features\n- impute missing values in features from the raw dataset and in created features\n- encode non-numeric features from the raw dataset as numeric features (in the feature-target dataset)\n\nA preprocessing pipeline creates the feature-target dataset from which the train and test datasets will be created.\nThis is a potential source of what is called \"data leakage\"."],"metadata":{}},{"cell_type":"markdown","source":["## 2.1 Data leakage"],"metadata":{}},{"cell_type":"markdown","source":["Jason Brownlee, in \n[Data Leakage in Machine Learning](https://machinelearningmastery.com/data-leakage-machine-learning/)), \ndescribes data leakage as (paraphrased)\n> _information from outside the train dataset being used to create the model_\n\nHe references [Leakage, by Data Skeptic](http://dataskeptic.com/blog/episodes/2016/leakage) who states (paraphrased)\n> _any feature created with information not available when the model makes predictions is introducing data leakage_\n\nThere are several ways this may happen. Two are:\n1. information from the final evaluation of the test dataset is used to create the feature-target dataset or the train dataset\n1. information from the test dataset makes its way into the train dataset\n1. information from the future is stored with data time stamped in the past\n\nThe first item can be avoided by only evaluating the final model on the test dataset.\nThis statement can be reworded as, once a model has been evaluated on the test dataset then it is the final model.\nIt is acceptaable, and common practice, to _fit the train dataset to a `GridSearchCV` object initialized with an estimator pipeline and a parameter grid_ multiple times, and then compare the collection of output results (across multiple runs) to find the best model.  \n\nThe second item can be avoided by not including scaling, normalization, standardization, feature selection or dimensionality reduction in the preprocessing pipeline.\n(They should be included in the estimator pipeline.)\nImputing missing values with the mean/median of the dataset is not OK, \nbut imputing with static values chosen based on domain knowledge is OK.\n\nAny transformation (in the preprocessing pipeline) should be performed on a per value basis without information from or about the entire dataset.\nFor instance, changing a text field to lower case seems OK, \nbut selecting a variable based on how well it correlates to the target across the entire dataset (train and test) doesn't seem OK.\nFrom this perspective objects with fit methods are likely not acceptable as fit methods are designed to store information about the entire dataset.\n\nFinally,  we need to address lag variables.\nStephen Nawara describes lag variables in \n[Avoiding Data Leakage in Machine Learning](https://conlanscientific.com/posts/category/blog/post/avoiding-data-leakage-machine-learning/)\nas a remedy to avoid time series features that include same-day information about other time series features (using information in from the future) one of which may be the target."],"metadata":{}},{"cell_type":"markdown","source":["Features can be created by both the preprocessing pipeline and by the estimator pipeline.\n\nPreprocessing pipelines are used to create features whose creation would produce missing values (such as lag variables).\nThey are also used to create features whose creation doesn't involve any hyperparameters that would be tuned.\nIt is more computationally efficient to create features in the preprocessing pipeline as it is run only once. \n\nEstimator pipelines, on the other hand, are used to create features which are determined by hyperparameters.\nWhen this is the case these hyperparameters can be tuned to produce optimal models."],"metadata":{}},{"cell_type":"markdown","source":["## 3. Creating train-test dataset pairs"],"metadata":{}},{"cell_type":"markdown","source":["For each feature-target dataset create:\n- one train-test dataset pair \n- one estimator pipeline (see below)\n\nThe cardinal rule with regard to the train-test pairs is that predictions are created only once and only for one of the test datasets.\nThis allows multiple train datasets to be fit to `GridSearchCV` objects initialized with different estimator pipelines and parameter grids.\nThe resulting output from `GridSearchCV` can be compared.\nThen the best model can be evaluated on the corresponding test dataset."],"metadata":{}},{"cell_type":"markdown","source":["## 4. Estimator pipelines"],"metadata":{}},{"cell_type":"markdown","source":["The end result of the estimator pipeline is a model, which, of course, can make predictions."],"metadata":{}},{"cell_type":"markdown","source":["Estimator pipelines:\n- begin with a sequence of zero or more transformer objects, which either create features or select features \n- end with an estimator object\n- provide `fit`, `predict` and `score` methods"],"metadata":{}},{"cell_type":"markdown","source":["### 4.1 Components of the estimator pipeline"],"metadata":{}},{"cell_type":"markdown","source":["When creating an estimator pipeline there are several choices to make:\n1. which transformer objects to use\n1. which estimator object to use\n1. which scoring metric to use\n\nIn addition, and once the above choices are made, parameter grids can be chosen in order to investigate sets of hyperparameters.\nThis is usually performed with the goal of finding a model which produces the best predictions. \nThere are several techniques to do so.\nGrid search is the most straightforward, but there are several \"smarter\" more advanced optimization methods.\nSee the section titled \"Smart Hyperparameter Tuning\" in _Evaluating Machine Learning Models_."],"metadata":{}},{"cell_type":"markdown","source":["### 4.1 Feature creation"],"metadata":{}},{"cell_type":"markdown","source":["Features can be created by both the preprocessing pipeline and by the estimator pipeline.\n\nPreprocessing pipelines are used to create features whose creation would produce missing values (such as lag variables).\nThey are also used to create features whose creation doesn't involve any hyperparameters that would be tuned.\nIt is more computationally efficient to create features in the preprocessing pipeline as it is run only once. \n\nEstimator pipelines are used to create features where this creation is determined by hyperparameters.\nWhen this is the case these hyperparameters can be tuned to produce models that produce optimal predictions.\n\nThe objects of the `CountVectorizer` class are examples of transformers that create features and that are determined by hyperparameters.\nTwo of these hyperparameters are `stop_words` and `ngram_range` (of the init method).\n\nThe current project code places the `CountVectorizer` object in the preprocessing pipeline. It can be moved to the estimator pipeline so that its hyperparameters can be used to tune the model."],"metadata":{}},{"cell_type":"markdown","source":["### 4.2 Feature selection and feature reduction\n\nOften feature-target datasets have many features.\nSome features may have been present in the raw dataset and others may have been created in either of the pipelines.\nKDnuggets News (see __References__ above) lists several reasons why it may be more desirable to use fewer features in making good predictions:\n- Some features might be irrelevant \n- Some features might be redundant \n- Overfitting is more likely with many features\n- Models with fewer features are easier to understand\n\nFeature selection and feature reduction should be part of the training process, which means for our work that it should be part of the estimator pipeline. \nSee [External Validation](https://topepo.github.io/caret/feature-selection-overview.html#external-validation)\nfrom [The `caret` package](https://topepo.github.io/caret/index.html) \nand [An Introduction to Feature Selection, by Jason Brownlee](https://machinelearningmastery.com/an-introduction-to-feature-selection/).\n\nThere are two approaches to using fewer features:\n1. Feature selection\n2. Dimensionality reduction\n\nEach is explained in a separate section below."],"metadata":{}},{"cell_type":"markdown","source":["#### 4.2.1 Feature selection \nAs mentioned above, it is often desirable to reduce the number of features in a feature-target dataset. \nIt is not clear though which features to remove and which to keep.\nFor this reason several algorithms have been written to automate the process of selecting features. \nThey fall into three groups: \n1. __Based on the dataset__. Features are ranked based on per feature scores provided by a scoring function, which is often a statistical measurement. \n1. __Based on models built from the dataset__. Features are ranked, or chosen, based on their importance to the model. \n1. __Based on predictions of models (built from the dataset)__. Features are chosen based on their ability to reduce prediction error."],"metadata":{}},{"cell_type":"markdown","source":["##### 4.2.1.2 Dataset based \n\nThis collection of techniques involves:\n- Applying a scoring function to each feature (possibly in relation to the target feature)\n- Choosing features based on their rank with regard to these functions\n\nVariance is the most common scoring function used for feature selection.\nOnly features which exceed a given threshold are kept for further analysis. \nScikit-learn implements this technique with the \n[VarianceThreshold](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) \ntransformer class. \n\nTwo transformer classes\n[`SelectPercentile`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile)\nand \n[`SelectKBest`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n(implemented by Scikit-learn) take as input a scoring function and, when fit to a dataset, return the top percentile or number of features (respectively) based on the ranking provide by the scoring function.\n\nScikit-learn makes available these four scoring functions\n([`f_classif`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html),\n [`f_regression`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html),\n [`mutual_info_classif`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) and\n [`mutual_info_regression`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html))\nwhich score features in relation to, categorical or numeric, target variables.\nNote that a value of `0` indicates no linear dependence for [`f_regression`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html), but that a value of `0` indicates no dependence for [`mutual_info_regression`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html).\nAdditional scoring functions are supplied by Scikit-learn and can be written by the user."],"metadata":{}},{"cell_type":"markdown","source":["##### 4.2.1.3 Model based \n\nSome algorithms assess the importance of features during the process of building the model. \nThere are two basic types of these models. \nThe first produces weights for each feature that indicate their importance in relation to the model.\nThe second produces models that may not include all features and so effectively eliminate some features and select others. \nLASSO is the most common example of this (second) type of model.\n\nFeature selection of the first type, in Python, requires an estimator with a `fit` method that creates either a `coef_` or a `feature_importances_` attribute, which provides the weight for each feature. \nFeatures with higher weights are selected. \n\nThe two transformer classes (described below) implement different methods using these weights to choose features. \n- `SelectFromModel` chooses features whose weights surpass a given threshold\n- `RFE` iteratively prunes features with the least weight\n\nFeature selection of the second type is called _regularization_.\nA list of regularized models in R can be found in the\n[`caret` package documentation](https://topepo.github.io/caret/feature-selection-overview.html#models-with-built-in-feature-selection). \nThe regularized models in Scikit-learn models the linear models, tree-based models and support vector machine models."],"metadata":{}},{"cell_type":"markdown","source":["###### 4.2.1.3.1 Ranking (`SelectFromModel`)\n\nThe `SelectFromModel` transformer class takes as input an estimator class and a weight threshold,\nwhich is specified as a proportion of either the mean or median. \n\n\nThe `fit` method \n1. Takes as input a predictor array (and, optionally, a target array)\n1. Fits this/these array(s) to the estimator class \n\nThe `transform` method\n1. Takes as input a predictor array\n1. Returns those features (columns of the predictor array) whose weight, according to the estimator, is greater than the threshold\n\nScikit-learn documentation for `SelectFromModel` can be found at\n- http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html"],"metadata":{}},{"cell_type":"markdown","source":["###### 4.2.1.3.2 Recursive feature elimination (`RFE`)\n\nThe `RFE` transformer class takes as input an estimator class and the number of features to select. \n\nThe `fit` method\n1. Takes as input a predictor array (and, optionionally, a target array)\n1. Proceeds through the algorithm below\n1. The end result of the algorithm is a chosen subset of features\n\nThe `RFE` algorithm in brief:\n1. Fit the estimator to the entire predictor array\n1. Remove the feature(s) with the least weight \n1. If the number of features is equal to the number of features to select, then stop (otherwise continue)\n1. Fit the remaining features to the estimator, creating new weights (for each of the remaining features)\n1. Go to the second step\n\nThe `transform` method\n1. Takes as input a predictor array\n1. Returns those features of the predictor array determined by the `fit` method\n\nScikit-learn documentation for the `RFE` model can be found at\n- http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"],"metadata":{}},{"cell_type":"markdown","source":["##### 4.2.1.4 Prediction based\n\nThese techniques assess feature importance based on the ability (of those features) to decrease the error of a given model's predictions.\n\nOne approach (of this type) simply fits a model to every combination of features (an exhaustive search) and keeps the set of features for which the model produces the best predictions. This approach finds the optimal solution (set of features), but is rarely practical when the size of the original feature set is large. \n\nThree classes of techniques, _genetic algorithms_, _forward selection_, _backward elimination_, find sub-optimal solutions (without an exhaustive search through all feature combinations). \n- Genetic algorithms try small and large changes to the feature set to search the space of feature combinations\n- Forward selection incrementally adds features (to an initially empty feature set) that provide the greatest decrease in prediction error\n- Backward elimination removes features (from the original feature set) that provide the greatest decrease in prediction error\n\nThere are many libraries that implement these techniques. The following packages stand out based on completeness and clarity of their documentation.\n- [Sequential Feature Selector](https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/), by Sebastian Raschka, from [mlxtend documentation](https://rasbt.github.io/mlxtend/); see also this [mlxtend demo](https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html)\n- [Feature selection overview](https://topepo.github.io/caret/feature-selection-overview.html) of the [`caret` package in R](https://topepo.github.io/caret/index.html)"],"metadata":{}},{"cell_type":"markdown","source":["### 4.2.2 Dimensionality reduction\n\nDimensionality reduction techniques _replace_ an original set of features with a smaller set of new features and so does not perform feature _selection_.\nA few examples are:\n- PCA (principal component analysis)\n- NMF (non-negative matrix factorization)\n- Auto encoders (a deep learning technique)"],"metadata":{}},{"cell_type":"markdown","source":["## References\n\nWorkflows and pipelines\n- [Architecting a Machine Learning Pipeline, by Semi Koen](https://towardsdatascience.com/architecting-a-machine-learning-pipeline-a847f094d1c7)\n- [Evaluating Machine Learning Models, by Alice Zheng](https://learning.oreilly.com/library/view/evaluating-machine-learning/9781492048756/)\n- [Scikit-learn Model selection: choosing estimators and their parameters](https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html)\n- [Scikit-learn 4.3 Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html)\n\nTrain, validation and test datasets\n- [What is the Difference Between Test and Validation Datasets? by Jason Brownlee](https://machinelearningmastery.com/difference-test-validation-datasets/)\n- [Data Leakage in Machine Learning, by Jason Brownlee](https://machinelearningmastery.com/data-leakage-machine-learning/)\n- [Leakage, by Data Skeptic](http://dataskeptic.com/blog/episodes/2016/leakage)\n- [Leakage in Data Mining: Formulation, Detection, and Avoidance, by Kaufman, Rosset, and Perlich](https://www.cs.umb.edu/~ding/history/470_670_fall_2011/papers/cs670_Tran_PreferredPaper_LeakingInDataMining.pdf)\n- [Avoiding Data Leakage in Machine Learning by Stephen Nawara, PhD.](https://conlanscientific.com/posts/category/blog/post/avoiding-data-leakage-machine-learning/)\n- [Preparing and Cleaning Data for Machine Learning, by Dataquest Labs, Inc.](https://www.dataquest.io/blog/machine-learning-preparing-data/)\n\nFeature selection and dimensionality reduction\n- [Scikit-learn 1.13 Feature selection](https://scikit-learn.org/stable/modules/feature_selection.html`)\n- [Must-Know: Why it may be better to have fewer predictors in Machine Learning models?, by KDnuggets](https://www.kdnuggets.com/2017/04/must-know-fewer-predictors-machine-learning-models.html)\n- [4 ways to implement feature selection in Python for machine learning](https://hub.packtpub.com/4-ways-implement-feature-selection-python-machine-learning/), by Sugandha Lahoti\n- [A survey of dimensionality reduction techniques](https://arxiv.org/pdf/1403.2877.pdf), by  C.O.S. Sorzano, J. Vargas and A. Pascual‐Montano\n- [An Introduction to Feature Selection](https://machinelearningmastery.com/an-introduction-to-feature-selection/), by Jason Brownlee\n- [An Introduction to Variable and Feature Selection](http://jmlr.csail.mit.edu/papers/volume3/guyon03a/guyon03a.pdf), by Isabelle Guyon and Andre Elisseeff\n- [Dimensionality Reduction: A Comparative Review](https://lvdmaaten.github.io/publications/papers/TR_Dimensionality_Reduction_Review_2009.pdf), by Laurens van der Maaten, Eric Postma and Jaap van den Herik\n- [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/), by Jason Brownlee\n- [Feature Selection For Machine Learning in Python](https://machinelearningmastery.com/feature-selection-machine-learning-python/), by Jason Brownlee\n- [Feature Selection to Improve Accuracy and Decrease Training Time](https://machinelearningmastery.com/feature-selection-to-improve-accuracy-and-decrease-training-time/), by Jason Brownlee\n- `FeatureSelection` class: [Blog](https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0), \n  [GitHub](https://github.com/WillKoehrsen/feature-selector)\n- [Genetic algorithms for feature selection in Data Analytics](https://www.neuraldesigner.com/blog/genetic_algorithms_for_feature_selection), by Fernando Gómez and Alberto Quesada\n- [How to use Python to select the right variables for data science](https://www.dummies.com/programming/big-data/data-science/how-to-use-python-to-select-the-right-variables-for-data-science/), by John Paul Mueller, Luca Massaron\n- [Introduction to Feature Selection methods with an example (or how to select the right variables?)](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/), by Saurav Kaushik\n- Quora: How do I perform feature selection? [Olivier Grisel, contributor to the scikit-learn project](https://www.quora.com/How-do-I-perform-feature-selection)\n- [Review on wrapper feature selection approaches](https://ieeexplore.ieee.org/document/7745366/), by Naoual El Aboudi and Laila Benhlima\n- [The `caret` Package](https://topepo.github.io/caret/index.html), by Max Kuhn"],"metadata":{}},{"cell_type":"markdown","source":["__The End__"],"metadata":{}}],"metadata":{"name":"W12.1 ML workflows (1)","notebookId":1399479},"nbformat":4,"nbformat_minor":0}
